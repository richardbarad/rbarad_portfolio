[
  {
    "objectID": "posts/politics_python/python_hw2.html",
    "href": "posts/politics_python/python_hw2.html",
    "title": "Analyzing the Historical Voter Turnout for Primary Elections in Philadelphia",
    "section": "",
    "text": "This analysis was completed as part of a my python course at the University of Pennslyvania. The assignment required us to explore a dataset from the Open Philadelphia data portal and produce charts using matplotlib, altair, and seaborn.\nPrior to making the charts, I had to clean the data and merge together data from multiple different election cycles. In this homework assignment I also discuss some of the advantages and disadvantages of Altair, Matplotlib, and seaborn."
  },
  {
    "objectID": "posts/politics_python/python_hw2.html#overview",
    "href": "posts/politics_python/python_hw2.html#overview",
    "title": "Analyzing the Historical Voter Turnout for Primary Elections in Philadelphia",
    "section": "",
    "text": "This analysis was completed as part of a my python course at the University of Pennslyvania. The assignment required us to explore a dataset from the Open Philadelphia data portal and produce charts using matplotlib, altair, and seaborn.\nPrior to making the charts, I had to clean the data and merge together data from multiple different election cycles. In this homework assignment I also discuss some of the advantages and disadvantages of Altair, Matplotlib, and seaborn."
  },
  {
    "objectID": "posts/politics_python/python_hw2.html#import-libraries-and-read-data",
    "href": "posts/politics_python/python_hw2.html#import-libraries-and-read-data",
    "title": "Analyzing the Historical Voter Turnout for Primary Elections in Philadelphia",
    "section": "Import Libraries and Read Data",
    "text": "Import Libraries and Read Data\n\n\nCode\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport altair as alt\nimport os\nimport seaborn as sns\n\npath =  './/Data'\n\nfiles = os.listdir(path)\n\ndf_list =[]\n\nfor f in files:\n    df_list.append(pd.read_csv('.//Data//' + f))\n\nvote_data = pd.concat(df_list)"
  },
  {
    "objectID": "posts/politics_python/python_hw2.html#clean-data",
    "href": "posts/politics_python/python_hw2.html#clean-data",
    "title": "Analyzing the Historical Voter Turnout for Primary Elections in Philadelphia",
    "section": "Clean Data",
    "text": "Clean Data\n\n\nCode\n#Shorten election field to contain just the four digit year to make visuals cleaner\n\nvote_data['election'] = vote_data['election'].str[:5]\n\n\n\n\nCode\n#Group all voters who are not democrat or Republican into an Other group and aggregate other group together\n\nmajor_party = vote_data['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])\nvote_data.loc[~major_party,'political_party'] = 'OTHER'\nvote_data = vote_data.groupby(['precinct_code','precinct_description','election','political_party'],as_index=False)['voter_count'].sum()\nvote_data.head()\n\n\n\n\n\n\n\n\n\nprecinct_code\nprecinct_description\nelection\npolitical_party\nvoter_count\n\n\n\n\n0\n101\nPHILA WD 01 DIV 01\n2015\nDEMOCRATIC\n157\n\n\n1\n101\nPHILA WD 01 DIV 01\n2015\nOTHER\n7\n\n\n2\n101\nPHILA WD 01 DIV 01\n2015\nREPUBLICAN\n3\n\n\n3\n101\nPHILA WD 01 DIV 01\n2016\nDEMOCRATIC\n207\n\n\n4\n101\nPHILA WD 01 DIV 01\n2016\nOTHER\n13"
  },
  {
    "objectID": "posts/politics_python/python_hw2.html#matplotlib---total-voter-turnout-in-primary-elections-in-philadelphia",
    "href": "posts/politics_python/python_hw2.html#matplotlib---total-voter-turnout-in-primary-elections-in-philadelphia",
    "title": "Analyzing the Historical Voter Turnout for Primary Elections in Philadelphia",
    "section": "Matplotlib - Total Voter Turnout in Primary Elections in Philadelphia",
    "text": "Matplotlib - Total Voter Turnout in Primary Elections in Philadelphia\nI used matplotlib for this graph because matplotlib seems best suited for creating simple visulizations like bar graphs. Creating a bar graph using Matplotlib requires minimal code. For more complex visulizations like scatter plots, stacked bar graphs, and heatmaps another library would allow us to make the visulizations with less code.\n\n\nCode\n#Calculate total turnout per year\n\nturnout = vote_data.groupby('election',as_index=False)['voter_count'].sum()\n\nfrom matplotlib import ticker\n\nfig, ax = plt.subplots(figsize=(5, 3))\n\ndef addlabels(x,y):\n    for i in range(len(x)):\n        plt.text(i,y[i]-29000,y[i],ha = 'center')\n        \nax.bar(turnout['election'],turnout['voter_count'],width=0.6,color='#7a9dff')\n\nax.set_ylabel('Voter Turnout')\nax.set_xlabel('Election')\nax.yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:,.0f}\"))\nax.yaxis.grid(visible = 'True',linewidth=0.5,color='#e0e0e0')\nax.set_axisbelow(True)\n\naddlabels(turnout['election'],turnout['voter_count'])\n\nplt.title('Voter Turnout in Primary Elections in Philadelphia from 2015-2018')\nplt.show()\n\n\n\n\n\nThe matplotlib graph shows total voter turnout in each primary election from 2015-2018. From the graph we can observe that primary turnout is highest in 2016 which was the year of a presidential election. The second highest primary turnout was in 2015 which was the year of a mayoral election."
  },
  {
    "objectID": "posts/politics_python/python_hw2.html#make-boxplot-chart-using-seaborn",
    "href": "posts/politics_python/python_hw2.html#make-boxplot-chart-using-seaborn",
    "title": "Analyzing the Historical Voter Turnout for Primary Elections in Philadelphia",
    "section": "Make Boxplot Chart using Seaborn",
    "text": "Make Boxplot Chart using Seaborn\nThis chart is a boxplot which shows the distribution of the number of voters by precinct from 2015 to 2018 in primary elections. Boxplots are included for both Democrats and Republicans primaries. The middle line in the box represents the median value, the top of the box represents the upper quartile (Q3), while the bottom of the box represents the lower quartile (Q1) - in other words 50% of the data points are located within the box. The whiskers extending from the box extend to the minimum and maximum values for the number of voters. I used seaborn for this visulization because Seaborn library is designed for creating statistical diagrams like boxplots.\n\n\nCode\n# Filter to just registered Democratic and Republican Voters\nmajor_party = vote_data['political_party'].isin(['DEMOCRATIC','REPUBLICAN'])\nvote_data_major_party = vote_data.loc[major_party]\n\nfig, ax = plt.subplots(figsize=(5, 4))\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Draw a nested boxplot to show bills by day and time\nbox_plot = sns.boxplot(data=vote_data_major_party,\n                       x=\"election\", \n                       y=\"voter_count\",\n                       hue=\"political_party\",\n                       whis=(0,100),\n                       width=0.5,\n                      linewidth=0.7)\n\nax.set_ylabel('Voter Turnout by Precicinct')\nax.set_xlabel('Election')\nplt.title('Boxplot of Primary Voter Turnout By Precinct from 2015 - 2018 in Philadelphia')\nplt.show()\n\n\n\n\n\nThe boxplots show that this a wide variation in the number of voters voting in each precinct. This could be because the number of people living in each precinct is variable or it could also be caused by variable turnout across the precincts. Across all years, the number of voters voting in Democratic pimary elections in Philadelphia is much higher than the number of voters voting in Republican elections. The median voter turnout was highest in the 2016 primary for both Democrats and Republicans. For Democrats, the value for the first/lower quartile (Q1), is higher than the value of the third/upper quartile (Q3) in 2017 and 2018 indicating a large jump in voter turnout in 2016 durring the presidential election year."
  },
  {
    "objectID": "posts/politics_python/python_hw2.html#altair-visualizations",
    "href": "posts/politics_python/python_hw2.html#altair-visualizations",
    "title": "Analyzing the Historical Voter Turnout for Primary Elections in Philadelphia",
    "section": "Altair Visualizations",
    "text": "Altair Visualizations\n\nAltair Chart 1 - Percent of Total Primary Voters by Political Party\n\n\nCode\nalt.data_transformers.disable_max_rows()\nsource = vote_data\n\nalt.Chart(source).transform_aggregate(\n    total1='sum(voter_count)',\n    groupby=['election', 'political_party']\n).transform_joinaggregate(\n    total2='sum(total1)',\n    groupby=['election']  \n).transform_calculate(\n    frac=alt.datum.total1 / alt.datum.total2\n).mark_bar().encode(\n    x=alt.X('total1:Q').stack(\"normalize\").title('Percent of Total Voters'),\n    y='election',\n    color='political_party',\n    tooltip=[alt.Tooltip('political_party', title='Poltical Party'),\n             alt.Tooltip('total1:Q', title='Number of Voters', format=',.0f'),\n             alt.Tooltip('frac:Q', title='Percent of Voters', format='.0%')\n            ]\n).properties(height=alt.Step(30),title='Percent of total Voters voting in each primary election')\n\n\n\n\n\n\n\n\nThis chart shows the percent of total voters voting in democratic, republican, and other primaries from 2015 to 2018. The charts indicates that from 2015-2018 between 90 and 85% of voters in Philadelphia voted in the democratic primary. Between 7 and 12% of primary voters voted in the republican primary. The percent of primary voters who voted in the republican primary is highest in 2016 when 12 percent of voteres voted in the Republican primary. Based on previous graphs, we know that both democratic and republican turnout increased in 2016. This graph shows us that the percent increase in turnout in 2016 was higher for the Republican party. We know this becase the percentage of total voters who are Democrats declined in 2016 despite the increase in democratic turnout.\n\n\nAltair Chart 2 - Total Voter Turnout in Primary Elections\n\n\nCode\nalt.data_transformers.disable_max_rows()\nsource = vote_data\n\nalt.Chart(source).mark_bar().encode(\n    x='election',\n    y=alt.Y('sum(voter_count)').title('Number of Voters'),\n    color='political_party',\n    tooltip=[alt.Tooltip('political_party', title='Poltical Party'),\n             alt.Tooltip('sum(voter_count)', title='Number of Voters', format=',.0f')\n            ]\n).properties(width=alt.Step(90),title='Voter Turnout from 2015-2018 in Primary Elections')\n\n\n\n\n\n\n\n\nThis chart show voter turnout by party from 2015 to 2018 in primary elections. For both of the major parties, turnout was highest in 2016 which was the year of the presidential election. The second highest turnout year was 2015, which was the year of a mayoral election. Total turnout was similar for the 2017 and 2018 primaries. However, the democratic party had higher turnout in 2017 than 2018. Conversely, republican turnout was higher in 2018 than 2017.\n\n\nAltair Chart 3 - Percent of Voters Voting as Democrats and Republicans by Ward\n\nData Processing\n\n\nCode\n# Calculate Number of Voters by Ward for each election / political party combination.\n\nvote_data['Ward'] = vote_data['precinct_description'].str[6:11]\nvote_data_ward = vote_data.groupby(['Ward','election','political_party'],as_index=False)['voter_count'].sum()\n\n# Calculate Percent of Residents in each ward voting for each party by election\n\nvote_data_total = vote_data_ward.groupby(['Ward','election'],as_index=False)['voter_count'].sum()\nvote_data_total.rename({'voter_count':'total_votes'},axis=1,inplace=True)\n\nvote_w_total = vote_data_ward.merge(vote_data_total,on=['Ward','election'])\nvote_w_total['pct'] = (vote_w_total['voter_count'] / vote_w_total['total_votes'])\n\n#Pivot Data\n\nvote_w_total_pivot = vote_w_total.pivot(index=['Ward','election','total_votes'],columns='political_party',values='pct').reset_index()\nvote_w_total_pivot.head()\n\n\n\n\n\n\n\n\npolitical_party\nWard\nelection\ntotal_votes\nDEMOCRATIC\nOTHER\nREPUBLICAN\n\n\n\n\n0\nWD 01\n2015\n3848\n0.901767\n0.041580\n0.056653\n\n\n1\nWD 01\n2016\n5661\n0.869988\n0.036566\n0.093446\n\n\n2\nWD 01\n2017\n3358\n0.911257\n0.040203\n0.048541\n\n\n3\nWD 01\n2018\n3704\n0.904698\n0.029428\n0.065875\n\n\n4\nWD 02\n2015\n5540\n0.901444\n0.041516\n0.057040\n\n\n\n\n\n\n\n\n\nMake Chart\n\n\nCode\nbrush = alt.selection_interval()\n\n(\n    alt.Chart(vote_w_total_pivot).transform_calculate(\n    x='datum.DEMOCRATIC * 100',\n    y='datum.REPUBLICAN * 100'\n).mark_point()\n    .encode(\n        x=alt.X(\"x:Q\", scale=alt.Scale(zero=True),title='Percent Voting as Democrat'),\n        y=alt.Y(\"y:Q\", scale=alt.Scale(zero=True,domainMax=50),title='Percent Voting in Republican'),\n        color=alt.condition(brush, \"election:N\", alt.value(\"lightgray\")),\n        tooltip=[alt.Tooltip('Ward',title='Ward'),\n                 alt.Tooltip('total_votes',title='Total Turnout'),\n                 alt.Tooltip('DEMOCRATIC', title=\"% of Voters Voting in Democratic Primary\",format='.2%'), \n                 alt.Tooltip('REPUBLICAN', title=\"% of Voters Voting in Republican Primary\",format='.2%')]\n    )\n    .properties(width=200, height=200)\n    .facet(column=\"election\")\n    .add_params(brush)\n)\n\n\n\n\n\n\n\n\nThese charts show the percent of voters by Ward voting in the democratic primary and the republican primary. Across all wards in Philadelphia more than 50 percent of voters voted in the Democratic primary. This pattern holds true across all four of the analyzed election years. In most wards, more than 80 percent of voters voted in the democratic primary. There are several wards were the percentage of voters who vote in republican primaries is consistently high compared to the rest of the city - this includes Wards 66, 64, 63 and 45. In all four of the wards listed the percentage of voters voting in the republican primary is greater than 25 percent in all four of the analyzed election years.\n\n\nAltair Chart Four - Two Chart Dashboard\nThis altair dashboard includes a heatmap and a scatter plot. To use the dashboard, click on any box in the heat map. All points in the scatter plot will turn grey excepte for the selected point. You can hold down shift on your keyboard to select multiple points. For example, a user might want to select all data points for one ward to see what percent of Voters Voted in the Democratic Primary in the ward they level in. For example a user could select the four data points for Ward 66 in the heat map and see that Ward 66 has the lowest percentage of voters voting in democratic primary across the city.\n\n\nCode\nselection = alt.selection_point()\n\npoints = (\n    alt.Chart().transform_calculate(\n    x='datum.DEMOCRATIC * 100'\n)\n    .mark_point()\n    .encode(\n        x=alt.X(\"total_votes:Q\", scale=alt.Scale(zero=True),title='Number of Voters'),\n        y=alt.Y(\"x:Q\", scale=alt.Scale(zero=True), title='Percent of Voters Voting in Democratic Primary'),\n        color=alt.condition(selection,\"election:N\",alt.value(\"lightgray\")),\n        tooltip=[alt.Tooltip('Ward',title='Ward'),\n                 alt.Tooltip('election',title='Election Year'),\n                 alt.Tooltip('total_votes',title='Total Turnout'),\n                 alt.Tooltip('DEMOCRATIC', title=\"% of Voters Voting in Democratic Primary\",format='.0%'), \n                 alt.Tooltip('REPUBLICAN', title=\"% of Voters Voting in Republican Primary\",format='.0%')]\n    )\n    .properties(width=650, height=400)\n)\n\nheatmap = (\n    alt.Chart()\n    .mark_rect()\n    .encode(\n    x='Ward:O',\n    y='election:O',\n    color='total_votes:Q',\n    opacity=alt.condition(selection,'1',alt.value(0.2))\n    )\n    .add_params(selection)\n    .properties(width=650)\n)\n\nchart = alt.vconcat(points, heatmap, data=vote_w_total_pivot)\n\nchart"
  },
  {
    "objectID": "posts/philly_flood_amenity_mapping/2-amenity-analysis.html",
    "href": "posts/philly_flood_amenity_mapping/2-amenity-analysis.html",
    "title": "Mapping Infrastructure Vulnerability to Flooding in Philadelphia",
    "section": "",
    "text": "This analysis looks at the number of amenities and buildings located within the flood hazard area in the city of Philadelphia. Amenity and building data is downloaded from Open Street Maps, and I also determine the planning district that each amenity is located in. I present the results using interactive maps created using the geopandas explore() function and charts created using the altair package.\nThis analysis is also availble as a PDF report."
  },
  {
    "objectID": "posts/philly_flood_amenity_mapping/2-amenity-analysis.html#data-visualization",
    "href": "posts/philly_flood_amenity_mapping/2-amenity-analysis.html#data-visualization",
    "title": "Mapping Infrastructure Vulnerability to Flooding in Philadelphia",
    "section": "Data Visualization",
    "text": "Data Visualization\nThe chart below shows the percentage of each amenity located in the 100 year flood zone, the 500 year flood zone, and amenities located outside of the flood plain. Pubs, fire stations, and fast food restaurants have the highest percentage of points located in the flood hazard zone. The high percentage of fire stations located in the flood zone is concerning, due to the imortant role this feature plays in emergancy response.\n\n\nCode\ndomain = ['100 YEAR', '500 YEAR', 'Not in Floodplain']\nrange1 = ['blue','lightblue','lightgrey']\n\nalt.Chart(amenities_join).mark_bar().encode(\n    alt.X('amenity').title('Amenity Type'),\n    alt.Y('count(amenity)').title('Percent of Total').stack(\"normalize\"),\n    alt.Color('ZONE').scale(domain=domain, range=range1),\n    tooltip=['amenity','count(amenity)','ZONE']\n).properties(\n    width=400,\n    height=300,\n    title='Percent of Citywide Amenities Located in Areas Vulnerable to Flooding'\n)\n\n\n\n\n\n\n\n\nNext, I create a copy of the amenities dataframe which just includes amenities located in the flood hazard zone.\n\n\nCode\nfilt = amenities_join['ZONE'] != 'Not in Floodplain'\namenities_flood = amenities_join.loc[filt]\n\n\nThe interactive map below shows the location of amenities which are located in the flood hazard zone. The amenities are colored according to the amenity type. Hover over a point to view more information about the amenity.\n\n\nCode\namenities_flood.explore(column='amenity',tiles='CartoDB positron')\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe chart below shows the number of amenities by amenity type located in the flood hazard area. The color allows for distinguishing between amenities in the 100 year and 500 year flood hazard zones. There are a large number of restaurants and fast food establishments located in flood hazard areas.\n\n\nCode\ndomain1 = ['100 YEAR','500 YEAR']\nrange2 = ['blue','lightblue']\n\nalt.Chart(amenities_flood).mark_bar().encode(\n    alt.X('amenity').title('Amenity Type'),\n    alt.Y('count(amenity)').title('Total'),\n    alt.Color('ZONE').scale(domain=domain1, range=range2),\n    tooltip=['amenity','count(amenity)','ZONE']\n).properties(\n    width=400,\n    height=300,\n    title='Number of Amenities Located in Areas Vulnerable to Flooding'\n)"
  },
  {
    "objectID": "posts/FEWS_NET_GDHI/GDHI.html",
    "href": "posts/FEWS_NET_GDHI/GDHI.html",
    "title": "Global Dashboard for Hostspot Indentification - Famine Early Warning Systems Network",
    "section": "",
    "text": "Overview\nThe Global Dashboard for Hotspot Identification (GDHI) is a screening tool used for by the Famine Early Warning System Network to identify potential food security areas of concern in the Horn of Africa. The GDHI provides FEWS NET with a rough estimate of potential food security durring the upcoming year. In my role as a Livelihood Monitoring Analyst at FEWS NET I was responsible for helping to build and run the GDHI on a monthly basis. The GDHI was developed as an Excel based tool, and historically analysts had to review the results of the analysis by e-mailing an Excel file with the results to everyone who needed them. In order to make the results more accessible I set-up a data visualization system to display the results using Microsoft Power BI and ArcGIS Online. Additionally, I developed a Python script that reads the results from an Excel file, transforms the results into the formats needed by Power BI and ArcGIS Online, and publishes the results. Browse below to view the python script, ArcGIS Online Maps, and Power BI Dashboard. Links to view the Power BI Dashboard and ArcGIS Online Maps in full screen are available above.\nA link to a video overview of the GDHI is also included above. Please watch the video if you are interested in learning more about how the GDHI works and how FEWS NET uses the map and dashboard tools I created.\n\n\nPython Script\nThe script below was written to automate the process of publishing the GDHI results to Power BI and ArcGIS Online. The script is written using arcpy, pandas, and datetime libraries and includes the following steps:\n\nUpdate the ArcGIS Online Feature class\n\nExtracts the GDHI results from excel file and imports the results to Pandas Dataframe.\nMerge together the Ethiopia results with the Kenya, Uganda, and Somalia GDHI results - Ethiopia results are stored in a separate file.\nExports pandas data frame to a .csv file, join .csv to a feature class, and export the joined result.\nSets appropriate aliases for the feature class fields\nAdd feature class to a Pro project, and remove feature class from previous month\nPublish updated ArcGIS Pro map to ArcGIS Online\n\nUpdate the csv files used in the Power BI Dashboard:\n\nGet current outlook and year from Excel Interface file\nCreate a dictionary with the time range associated with each analysis quarter based on the selected outlook and year run\nFlatten results using Pandas to create a flat file on population by Phase, by quarter for each area of analysis - export CSV results\nFlatten results using Pandas to create a flat file on MT needs by quarter for ach area of analysis - export CSV results\n\n\nClick the code button bellow to view the full script!\n\n\nCode\nimport os\nimport pandas as pd\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport arcpy\nimport openpyxl\nfrom arcgis.gis import GIS\n\narcpy.env.overwriteOutput = True\n\n#This is  the directory where the GDHI files for the current month are saved\nKE_SO_UG_GDHI=r'C:\\Users\\rbarad\\OneDrive - Chemonics\\10.GDHI\\01.EA_Monthly_Runs\\02.GDHI-tool\\2023\\05_RB\\KE_UG_SO\\run2' #Path to GDHI Excel files for KE, UG, SO for 2nd run\nET_GDHI=r'C:\\Users\\rbarad\\OneDrive - Chemonics\\10.GDHI\\01.EA_Monthly_Runs\\02.GDHI-tool\\2023\\05_RB\\ET\\GDHI_Outlook' #Path to GDHI results files for ET\nET_GDHI_NOPSNP='NatLIAS_res_summ_Outlook_NOPSNP.xlsm' #Name of results file without PSNP\nET_GDHI_PSNP='NatLIAS_res_summ_Outlook.xlsm' #Name of results file with PSNP\n\n#Edit this to select when adminstrative areas in Ethiopia should be greyed out\nadmin2_units = ['Zone 1','Zone 2','Zone 4','North Gondar', 'Wag Himra', 'North Wollo'] #Must match spelling used in shapefile\nadmin1_units = ['Tigray'] #Must match spelling used in shapefile, if you want to exclude an entire admin1 unit specify it here\n\n#Set month and year when analysis is for\nmonth=5\nyear=2023\n\nparams= {'input_featureclass':'SO_UG_KE_ET_GDHI_Admin_LHZ_simp', #Name of Feature class to join GDHI results to - must be stored in Pro Project GDB which is specified as the arcpy.env.workspace\n         'sharepoint_folder':r'C:\\Users\\rbarad\\OneDrive - Chemonics\\10.GDHI\\01.EA_Monthly_Runs\\01.SharePoint', #Location of Share Point folder containing ArcGIS Pro Project and Power BI Files\n         'username':'XXXX', #username for AGOL\n         'password':'XXXXXX', #Password goes here\n         'service':'GDHI_results'} #Name of service in AGOL to overwrite\n\noutlookstart = {1: 1,\n                2: 4,\n                3: 10}\n\n#Set the file path to the sharepoint folder, Project GDB, and Pro Project\nsharepoint_folder = params['sharepoint_folder']\narcpy.env.workspace=os.path.join(sharepoint_folder,'GDHI_Results.gdb') #Set arcgis workspace\npro_project = os.path.join(sharepoint_folder,'GDHI_Results.aprx') #Set path to project\n\n#Dictionary Contains Aliases to assign to each column in the feature class\nalias_list= {'Q1_IPC_Max': 'Q1 Max Indicative Household Phase','Q1_IPC_Area': 'Q1 Indicative Area Phase','Q1_3plus': 'Q1 Pop in IPC Phase 3+','Q1_MT': 'Q1 Metric Tons of aid',\n             'Q2_IPC_Max': 'Q2 Max Indicative Household Phase','Q2_IPC_Area': 'Q2 Indicative Area Phase','Q2_3plus': 'Q2 Pop in IPC Phase 3+','Q2_MT': 'Q2 Metric Tons of aid',\n             'Q3_IPC_Max': 'Q3 Max Indicative Household Phase','Q3_IPC_Area': 'Q3 Indicative Area Phase','Q3_3plus': 'Q3 Pop in IPC Phase 3+', 'Q3_MT': 'Q3 Metric Tons of aid',\n             'IPC_Max': 'Highest Indicative Household Phase','IPC_Area_Max':'Highest Indicative Area Phase', 'IPC_Area_Avg': 'Average Indicative Phase','MT_Total': 'Total Metric tons (Q1 - Q3)',\n             'Total_pop':'Total Population','Kg_Per_capita':'Kilograms per capita'}\n\n#Class of functions to read results into a pandas dataframes\nclass read_merge_data():\n    \n    def read_ke_so_ug(kesoug_results):\n        print('Read KE,SO,UG Results Data')\n        os.chdir(kesoug_results)\n        results = pd.DataFrame(pd.read_excel('NatLIAS_res_summ.xlsx',sheet_name='Mapping',skiprows=1,nrows=311)) #Read results from GDHI into a Dataframe\n        return results\n    \n    def read_et(et_folder,filename):\n        print('Read ET Results Data')\n        os.chdir(et_folder)\n        results = pd.DataFrame(pd.read_excel(filename,sheet_name='Mapping_Meth2',skiprows=1,nrows=875)) #Read results from GDHI into a Dataframe\n        results=results[results['FNID'].notnull()]\n        results = results[~((results['Admin2'].isin(admin2_units)) | (results['Admin1'].isin(admin1_units)))] #Remove Zone 1, Zone 2, and Zone 4 of Afar and Tigray since GDHI Not Valid there\n        results['COUNTRY'] = 'Ethiopia'\n        return results\n    \n    def merge(kesoug_results,et_folder,et_filename):\n        results1 = read_merge_data.read_ke_so_ug(kesoug_results)\n        results2 = read_merge_data.read_et(et_folder,et_filename)\n        print('Merge Data')\n        results = pd.concat([results1,results2],ignore_index=True)\n        return results\n\nresults = read_merge_data.merge(KE_SO_UG_GDHI,ET_GDHI,ET_GDHI_PSNP) #Read results with PSNP\nresults_NOPSNP = read_merge_data.read_et(ET_GDHI,ET_GDHI_NOPSNP) #Read results without PSNP\n\ndef create_quarter_IPC_list(): #Function creates a list of collumns names for the combinations of IPC Phases and quarter (i.e: Q1_IPC1, Q1_IPC2, Q1_IPC3, Q1_IPC4, Q1_IPC5, Q2_IPC1, etc.) \n    quarters=['Q1','Q2','Q3']\n    quarters_phase_list = []\n    IPC_Phase = range(1,6)\n    for q in quarters:\n        for p in IPC_Phase:\n            quarter_phase = q + '_IPC' + str(p)\n            quarters_phase_list.append(quarter_phase)\n    return quarters_phase_list\n\ndef create_quarter_variable_list(): #Function create list of data column names for each quarter - includes MT per quarter, Area Phase Clasification, Highest Phase Classification, and Metric Tons (MT) \n    quarters=['Q1','Q2','Q3']\n    fields = ['IPC_Max','IPC_Area','3plus','MT']\n    quarter_field_list = []\n    for q in quarters:\n        for f in fields:\n            field = q + \"_\" + f\n            quarter_field_list.append(field)\n    return quarter_field_list\n    \ndef create_results_featureclass(df,output_name): #Function to create featureclass, 1st input is a df containing GDHI results, second input is name of output featureclass\n    print(\"Create Featureclass for GDHI results...\")\n    #Save copy of GDHI Mapping units in memory\n    gdhi_shapes = os.path.join('in_memory',params['input_featureclass'])\n    arcpy.management.CopyFeatures(params[\"input_featureclass\"], gdhi_shapes) #Create copy of GDHI shapes in-memory\n    #Convert GDHI results to a .csv and join results to in memory featureclass, export featureclass to disk, delete .csv once complete\n    df.to_csv('NATLIAS_results.csv')\n    results_csv= os.getcwd() + os.sep + \"NATLIAS_results.csv\"\n    results_table = arcpy.conversion.TableToTable(results_csv, arcpy.env.workspace, 'results') #Had to convert csv to a table because JoinField Function was not working with .csv in Pro 3.0.\n    arcpy.JoinField_management(gdhi_shapes, 'FNID', results_table, 'FNID',fields_join) #Append data to in-memory featureclass\n    arcpy.management.CopyFeatures(gdhi_shapes, output_name)\n    arcpy.Delete_management(\"results\")\n    os.remove(results_csv)\n\ndef set_aliases(output): #Set aliases\n    for field in fields_join:\n        print(\"Update Alias for \" + field)\n        arcpy.AlterField_management(output, field, new_field_alias=alias_list[field])\n\n#Create featureclass for ESRI story map with & without PSNP using defined functions\nfields_join = create_quarter_variable_list() + ['IPC_Max','IPC_Area_Max','IPC_Area_Avg','MT_Total','Total_pop','Kg_Per_capita'] #Create list of quarterly data fields using function and add average variables to list\nmonth_name = datetime.date(year, month, 1).strftime('%Y_%m')  #Create string based on month and year to use in featureclass names\n\noutput_name = 'GDHI_results_' + month_name #Set name of output feature class with PSNP\ncreate_results_featureclass(results,output_name) #Create feature class for results with PSNP\nset_aliases(output_name) #Set alias names in feature class\n\noutput_name_NOPSNP = 'GDHI_results_' + month_name + 'NOPSNP' #Set name of output feature class without PSNP\ncreate_results_featureclass(results_NOPSNP,output_name_NOPSNP) #Create feature class for results without PSNP\nset_aliases(output_name_NOPSNP) #Set alias names in feature class\n\n#Create variables for Pro Project and Map in Pro Project\naprx = arcpy.mp.ArcGISProject(pro_project)\naprxMap = aprx.listMaps(\"Map\")[0] \n\ndef update_pro_project():\n    #Add new results Featureclasses to the ArcGIS Pro Project - rename layers, but first remove old GDHI results layer from map so that map only includes one layer.\n    print(\"Update Pro Project...\")\n    lyr_path_PSNP = os.path.join(arcpy.env.workspace,output_name)\n    lyr_path_noPSNP = os.path.join(arcpy.env.workspace,output_name_NOPSNP)\n    for lyr in aprxMap.listLayers(): #Remove existing layers    \n        aprxMap.removeLayer(lyr)\n    aprxMap.addDataFromPath(lyr_path_PSNP) #Add layer\n    lyr = aprxMap.listLayers()[0] #Select first and only layer in map\n    lyr.name = 'GDHI_results' #Rename selected layer to 'GDHI_results'\n    aprxMap.addDataFromPath(lyr_path_noPSNP) #Add no PSNP Layer to map\n    move_lyr = aprxMap.listLayers('*PSNP')[0] #Select no PSNP layer\n    aprxMap.moveLayer(lyr, move_lyr, 'AFTER') #Move PSNP to be the second layer in map to keep same order\n    move_lyr.name = 'GDHI_results_NOPSNP' #Rename selected layer to 'GDHI_results_NOPSNP' to keep name the same\n    aprx.save()\n    print(\"Pro Project Updated\")\n\ndef update_AGOL():\n    #Sign in to ArcGIS Online\n    print(\"Sign in to ArcGIS Online\")\n    gis = GIS('https://www.arcgis.com', params['username'], params['password'])\n    # Set sharing draft and service definition file names\n    service = params['service']\n    sddraft_filename = os.path.join(sharepoint_folder, service + \".sddraft\")\n    sd_filename = os.path.join(sharepoint_folder, service + \".sd\")\n    # Create FeatureSharingDraft and set service properties\n    print(\"Create Sharing Draft and Service Defintion Files...\")\n    sharing_draft = aprxMap.getWebLayerSharingDraft(\"HOSTING_SERVER\", \"FEATURE\", service)\n    sharing_draft.summary = \"Results of the GDHI for \" + datetime.date(year, month, 1).strftime('%B %Y')\n    sharing_draft.overwriteExistingService = True\n    sharing_draft.portalFolder = '01. GDHI'\n    # Create Service Definition Draft file and service definition\n    sharing_draft.exportToSDDraft(sddraft_filename)\n    arcpy.StageService_server(sddraft_filename, sd_filename)\n    # Find the Service definition, update it, publish /w overwrite and set sharing and metadata\n    print(\"Search for original SD on portal…\")\n    searchData = gis.content.search(query=\"title:\"+ service + \" AND owner: \" + 'FEWS_NET', item_type=\"Service Definition\")\n    for search in searchData:\n        print(search)\n        if search.title== service:\n            print(\"Found SD: {}, ID: {} Uploading and overwriting…\".format(search.title, search.id))\n            search.update(data=sd_filename)\n            print(\"Overwriting existing feature service…\")\n            fs = search.publish(overwrite=True)\n            print(\"Finished updating: {} – ID: {}\".format(fs.title, fs.id))\n        else: \n            pass\n            print('Pass item in list')\n\n#Update Pro project and publish feature class to AGOL.\nupdate_pro_project()\nupdate_AGOL()\n#arcpy.Delete_management(\"in_memory\") #Clear arcgis memory space\n\n#Rest of script creates csv files which are used in Power Bi. Will need to open Power BI and update data source after script copmletes\n\ndef get_outlook_year_from_Excel(): #Get the outlook and year of analysis from the SO, UG, KE GDHI file.\n    os.chdir(KE_SO_UG_GDHI)\n    book = openpyxl.load_workbook('NatLIAS_interface.xlsm')\n    sheet = book.active\n    year = sheet['E9'].value\n    outlook = sheet['E7'].value\n    return[year,outlook]\n\ndef generate_ranges(): #Generate month ranges for each quarter and write results to a python dictionary, subsequently used in IPC_Phase_Clean() and IPC_MT_Clean() functions to get the month ranges for eqch quarter\n    dictionary = {}    \n    print (\"Gernerate date range for each quarter, based on selected outlook\")\n    date = get_outlook_year_from_Excel()\n    start_date= datetime.date(date[0], outlookstart[date[1]], 1) #Convert number representing month from outlook start to a date based on year, and start month of selected GDHI run\n    dictionary['Q1']= '(' + start_date.strftime(\"%b. %y\") + ' - ' + (start_date + relativedelta(months=2)).strftime(\"%b. %y\") + ')'\n    dictionary['Q2']= '(' + (start_date + relativedelta(months=3)).strftime(\"%b. %y\") + ' - ' + (start_date + relativedelta(months=5)).strftime(\"%b. %y\") + ')'\n    dictionary['Q3']= '(' + (start_date + relativedelta(months=6)).strftime(\"%b. %y\") + ' - ' + (start_date + relativedelta(months=8)).strftime(\"%b. %y\") + ')'\n    return dictionary\n\ndef IPC_Phase_Clean(df,output_name): #Flatten to create a file on population by Phase, per quarter\n   print(\"Create File for IPC Phase by Quarter\")\n   try: #This try logic is necessary because the data with PSNP does not include an LH Zone Column since it just for Ethiopia\n       df_filt = df[~df['LH Zone'].isin(['BDA','SO19','KMO'])] #Remove urban results (only needed for results with Somalia included)\n       results_org = df_filt.melt(id_vars=['FNID','COUNTRY','Admin1','Admin2','Admin3','LH Zone','Total_pop'],value_vars=create_quarter_IPC_list(),value_name='Pop',var_name='Quarter_Phase') \n   except:\n       results_org = df.melt(id_vars=['FNID','COUNTRY','Admin1','Admin2','Admin3','Total_pop'],value_vars=create_quarter_IPC_list(),value_name='Pop',var_name='Quarter_Phase')\n   results_org['Quarter'] = results_org['Quarter_Phase'].str.split(\"_\",n = 1, expand = True)[0]\n   results_org['Phase'] = results_org['Quarter_Phase'].str.split(\"_\",n = 1, expand = True)[1]\n   results_org['Quarter'] = results_org['Quarter'] + ' ' + results_org['Quarter'].map(quartertimeranges)\n   results_org['Pop'] = results_org['Pop'].round(0) #round to nearest whole person since you can not have half a person\n   results_org.drop(labels='Quarter_Phase',axis=1,inplace=True)\n   try:\n       results_org.sort_values(['COUNTRY','Admin1','Admin2','Admin3','LH Zone','Quarter','Phase'],inplace=True)\n   except:\n       results_org.sort_values(['COUNTRY','Admin1','Admin2','Admin3','Quarter','Phase'],inplace=True)\n   results_org.to_csv(output_name)\n\ndef IPC_MT_Clean(df,output_name): #Flatten to create a file on MT by quarter\n    print(\"Create File for MT Needs by Quarter\")\n    try: #This try logic is necessary because the data with PSNP does not include an LH Zone Column since it just for Ethiopia\n        df_filt = df[~df['LH Zone'].isin(['BDA','SO19','KMO'])] #Remove urban results (only needed for results with Somalia included)\n        results_org_MT = df_filt.melt(id_vars=['FNID','COUNTRY','Admin1','Admin2','Admin3','LH Zone','Total_pop'],value_vars=['Q1_MT','Q2_MT','Q3_MT'],value_name='MT',var_name='Quarter_MT')\n    except:\n        results_org_MT = df.melt(id_vars=['FNID','COUNTRY','Admin1','Admin2','Admin3','Total_pop'],value_vars=['Q1_MT','Q2_MT','Q3_MT'],value_name='MT',var_name='Quarter_MT')\n    results_org_MT['Quarter'] = results_org_MT['Quarter_MT'].str.split(\"_\",n = 1, expand = True)[0]\n    results_org_MT['Quarter_detail'] = results_org_MT['Quarter'] + ' ' + results_org_MT['Quarter'].map(quartertimeranges)\n    results_org_MT.drop(labels='Quarter_MT',axis=1,inplace=True)\n    try: #This try logic is necessary because the data with PSNP does not include an LH Zone Column since it just for Ethiopia\n        results_org_MT.sort_values(['COUNTRY','Admin1','Admin2','Admin3','LH Zone','Quarter',],inplace=True)\n    except:\n        results_org_MT.sort_values(['COUNTRY','Admin1','Admin2','Admin3','Quarter',],inplace=True)\n    results_org_MT.to_csv(output_name)\n\n#Create csv files for PowerBI - save to Sharepoint folder\nquartertimeranges = generate_ranges()\nos.chdir(sharepoint_folder) # Change directory to SharePoint folder so that csv files are exported here - same path to csv file each time\n\n#Create Power BI files for results with PSNP\nIPC_Phase_Clean(results,'IPC_Phase.csv')\nIPC_MT_Clean(results,'MT_Needs.csv')\n\n#Create PowerBI files for results without PSNP\nIPC_Phase_Clean(results_NOPSNP,'IPC_Phase_noPSNP.csv')\nIPC_MT_Clean(results_NOPSNP,'MT_Needs_noPSNP.csv')\n\nprint(\"Script Complete\")\n\n\n\n\nArcGIS Online Maps\nThe maps below are created in ArcGIS Online to help showcase the results of the GDHI. The included maps show the highest indicative IPC Phase and the highest Indicative Household IPC Phase estimated by the GDHI across the analysis period. There is also a map showing the total Metric Tons of Assistance needed to fill deficits across the analysis period. The maps are combined into a single application using instant app tools available in ArcGIS Online. Click on the arrows at the bottom of the page to toggle between the different available maps.\n\n\n\n\nPower BI Dashboard\nI created the Power BI Dashboard below to show the results of the GDHI Analysis. The Dashboard provides information on the estimated population in each IPC Phase and includes visualizations showing the average population in IPC Phase 2+ by administrative unit across the period of analysis. The dashboard also provides information on the estimate Metric Tons of food assistance needed to fill food security deficits by quarter. You can click on the arrows at the bottom of the dashboard to see results for different countries."
  },
  {
    "objectID": "posts/Bluebonnet/bluebonnet.html",
    "href": "posts/Bluebonnet/bluebonnet.html",
    "title": "Analyzing Fundraising Data for a Poltical Campaign",
    "section": "",
    "text": "Overview\nIn 2021 I was a fellow with Bluebonnet Data, and worked as a volunteer data analyst for a local political campaign. Through this fellowship I was trained on political data methods, such as how to work with census data, the voter file, and VAN.\nIn my volunteer role, I collaborated closely with the campaigns Fundraising Manager and developed python scripts to analyze their historical fundraising data which was stored in NGP. The campaign was specifically interested in understanding what contribution types were bringing in the largest amount of funds and determining if their fundraising for the current election cycle was keeping up with previous election cycles. Below I have included a presentation that shows the results of my analysis. All data analysis was carried out using Python and the python script used to run the data analysis and output charts is also included below.\n\n\nPresentation\n\n\n\n\nScript\n\n\nCode\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Sep  6 15:37:12 2021\n\n@author: richa\n\"\"\"\n\nimport os\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom matplotlib import dates as mdates\nimport numpy as np\nimport datetime\n\nos.chdir(r'..\\..\\data\\processed\\ngp_analysis')\n\nngp_excel = r'..\\..\\raw\\ngp\\ngp_all_3.xlsx'\n\nend_date = '2021-10-01'\n\nngp_excel_df = pd.read_excel(ngp_excel)\n\n# ACCES GOOGLE SHEET\n\nsource_code_group_url = 'https://docs.google.com/spreadsheets/d/1-aMlLj5F7NQQsxybPNbCIkjWeVi2gWQMNV_Z8OeDtxU/gviz/tq?tqx=out:csv&sheet=Sheet1'\nsource_code_group = pd.read_csv(source_code_group_url)\n\n#Clean data, create generlized type column by combining information from multiple columns\nngp_excel_df.loc[ngp_excel_df['Contribution ID'] == 2589300,'Source Code'] = 'Party'\nngp_excel_df.loc[ngp_excel_df['Contribution ID'] == 2589300,'Source Code Path'] = 'Organizations/Party'\nngp_excel_df = ngp_excel_df[ngp_excel_df['Date Received'] &lt; end_date]\nngp_excel_df['type'] = np.nan\nngp_excel_df.loc[ngp_excel_df['Payment Method'] == 'Cash','type'] = 'Cash'\nngp_excel_df.loc[ngp_excel_df['Payment Method'] == 'In-Kind','type'] = 'In-Kind'\nngp_excel_df.loc[ngp_excel_df['Contribution Type'] == 'In-kind Contribution','type'] = 'In-Kind'\nngp_excel_df = ngp_excel_df.merge(source_code_group, how='left',on=['Source Code','Source Code Path'])\n\nngp_excel_df['type'].fillna(ngp_excel_df['Source_Code_Gen'], inplace=True)\nngp_excel_df['type'].fillna('Other or Unkown', inplace=True)\nngp_excel_df['year'] = pd.DatetimeIndex(ngp_excel_df['Date Received']).year\nngp_excel_df['month'] = pd.DatetimeIndex(ngp_excel_df['Date Received']).month\nngp_excel_df.to_excel('ngp_data_class.xlsx')\n\ntypes = ngp_excel_df['type'].unique()\n\n#Get aggregate statistics by year and month\n\nos.chdir(r'..\\..\\..\\data\\processed\\ngp_analysis')\ndf_agg = ngp_excel_df.groupby(['year','month','type'],as_index=False)['Amount'].agg(['sum','count','mean'])\ndf_agg.reset_index(inplace=True)\ndf_agg['day'] = 1\ndf_agg['date'] = pd.to_datetime(df_agg[['year','month','day']])\ndf_agg.to_excel('donation_summary.xlsx')\n\n#Function to create chart showing donation count and sum by month for specific data\ndef create_count_sum_chart(df,sum_field,count_field,t):\n    fig, ax1 = plt.subplots()\n    ax1.plot(df.index,df[sum_field],alpha=0.5)\n    ax1.grid(axis='y',linewidth=0.5)\n    ax1.set_ylabel(\"Amounted Recieved Per Month\",color='blue')\n    ax1.set_xlabel(\"Month\")\n    ax1.set_ylim(bottom=0)\n    ax2 = ax1.twinx()\n    ax2.plot(df.index,df[count_field],color='red', alpha=0.5)\n    ax2.set_ylim(bottom=0)\n    ax2.set_ylabel(\"Number of Donations per month\",color='red')\n    one_month = mdates.MonthLocator(interval=1)\n    ax1.tick_params(axis='y', colors='blue')\n    ax2.tick_params(axis='y', colors='red')\n    ax1.xaxis.set_minor_locator(one_month)\n    three_month = mdates.MonthLocator(interval=3)\n    ax1.xaxis.set_major_locator(three_month)\n    ax1.set_xticklabels(ax.get_xticks(), rotation = 90)\n    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n    fig_name = t + '_sumandcountpermonth.png'\n    plt.title(\"Number of Donations from \" + t)\n    plt.savefig(fig_name, bbox_inches=\"tight\")\n    plt.show()\n\n#Function to create chart showing donation count by month for specific data\ndef create_sum_chart(df,field,title):\n    fig, ax = plt.subplots()\n    ax.plot(df.index,df[field])\n    ax.grid(axis='y',linewidth=0.5)\n    ax.set_ylabel(\"Amounted Recieved Per Month\")\n    ax.set_xlabel(\"Month\")\n    ax.set_ylim(bottom=0)\n    one_month = mdates.MonthLocator(interval=1)\n    ax.xaxis.set_minor_locator(one_month)\n    three_month = mdates.MonthLocator(interval=3)\n    ax.xaxis.set_major_locator(three_month)\n    fig_name = field + '_totalpermonth.png'\n    plt.title(\"Total Amount Donated from \" + title)\n    plt.xticks(rotation = 90)\n    plt.savefig(fig_name, bbox_inches=\"tight\")\n    plt.show()\n\n#Function to create chart showing donation sum by month for specific data\ndef create_count_chart(df,field,title):\n    fig, ax = plt.subplots()\n    ax.plot(df.index,df[field])\n    ax.set_ylabel(\"Number of Donations per month\")\n    ax.grid(axis='y',linewidth=0.5)\n    ax.set_xlabel(\"Month\")\n    ax.set_ylim(bottom=0)\n    one_month = mdates.MonthLocator(interval=1)\n    ax.xaxis.set_minor_locator(one_month)\n    three_month = mdates.MonthLocator(interval=3)\n    ax.xaxis.set_major_locator(three_month)\n    fig_name = field + '_countpermonth.png'\n    plt.title(\"Number of Donations per month from \" + title)\n    plt.xticks(rotation = 90)\n    plt.savefig(fig_name, bbox_inches=\"tight\")\n    plt.show()\n\n#Pivot data, seperate columns for each type of donation\ndf_agg_pivot = df_agg.pivot_table(index='date',columns='type',values=['count','sum'])\ndf_agg_pivot.fillna(0,inplace=True)\ndf_agg_pivot.columns = ['_'.join(x) for x in df_agg_pivot.columns]\ncount_list = []\nsum_list= []\n\nos.chdir(r'..\\..\\..\\reports\\figures\\ngp')\n\nfor t in types:\n    sum_field = 'sum_' + t\n    count_field = 'count_' + t\n    create_sum_chart(df_agg_pivot,sum_field,t)\n    create_count_chart(df_agg_pivot,count_field,t)\n    create_count_sum_chart(df_agg_pivot,sum_field,count_field,t)\n\nfor t in types:\n    count_list.append('count_' + t)\n    sum_list.append('sum_' + t)\n\ndf_agg_pivot['Total_Cash_onhand'] = (df_agg_pivot[sum_list].sum(axis=1)) - df_agg_pivot['sum_In-Kind']\ndf_agg_pivot['Count_Cash_Donation'] = (df_agg_pivot[count_list].sum(axis=1)) - df_agg_pivot['count_In-Kind']\n\ncreate_sum_chart(df_agg_pivot,'Total_Cash_onhand','All Monetary Contributions')\ncreate_count_chart(df_agg_pivot,'Count_Cash_Donation','All Monetary Contributions')\ncreate_count_sum_chart(df_agg_pivot,'Total_Cash_onhand','Count_Cash_Donation','All Monetary Contributions')\n\n\ndf_agg_cash_only = df_agg[df_agg['type'] != 'In-Kind']\ndf_agg_date_filt = df_agg_cash_only[df_agg_cash_only['year'].isin([2017,2019,2021])]\ndf_agg_pivot2 = df_agg_date_filt.pivot_table(index='month',columns='year',values='sum',aggfunc='sum')\ndf_agg_pivot2.loc[1,2017] = 0\ndf_agg_pivot2.loc[2,2021] = 0\ndf_agg_pivot2.loc[2,2019] = 0\nprint(df_agg_pivot2)\n\nfig, ax = plt.subplots()\nax.plot(df_agg_pivot2.index,df_agg_pivot2[2017],label='2017', alpha=0.5)\nax.plot(df_agg_pivot2.index,df_agg_pivot2[2019],label='2019', alpha=0.5)\nax.plot(df_agg_pivot2.index,df_agg_pivot2[2021],label='2021', alpha=0.5)\nax.set_xticks(np.arange(1,13))\nax.set_xticklabels(['Jan', 'Feb','Mar.','Apr.','May','Jun.','Jul.','Aug.','Sep.','.Oct','.Nov','.Dec'])\nax.set_ylabel(\"Total Cash Contributions\")\nax.grid(axis='y',linewidth=0.5)\nplt.title('$ Contributions by Month for 2017, 2019, 2021 Election Cycles')\nax.set_xlabel(\"Month\")\nplt.legend()\nplt.savefig('election_comparison.png', bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Work Samples",
    "section": "",
    "text": "Global Dashboard for Hostspot Indentification - Famine Early Warning Systems Network\n\n\n\n\n\n\n\nFood Security\n\n\nPython\n\n\nArcGIS\n\n\nPower BI\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nRichard Barad\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Restaurant Inspection Failures in Chicago\n\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nRichard Barad, Alex Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nDeveloping a Wind Suitability Map for Pennsylvania\n\n\n\n\n\n\n\nArcGIS\n\n\nEnvironment\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\nRichard Barad\n\n\n\n\n\n\n  \n\n\n\n\nMapping Infrastructure Vulnerability to Flooding in Philadelphia\n\n\n\n\n\n\n\nEnvironment\n\n\nFlooding\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nRichard Barad\n\n\n\n\n\n\n  \n\n\n\n\nMapping Population Vulnerability to Flooding in Philadelphia\n\n\n\n\n\n\n\nFlooding\n\n\nPython\n\n\nCensus\n\n\nEnvironment\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nRichard Barad\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing the Historical Voter Turnout for Primary Elections in Philadelphia\n\n\n\n\n\n\n\nPolitics\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nRichard Barad\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Demand for Bikeshare in Boston\n\n\n\n\n\n\n\nR\n\n\nTransportation\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\nRichard Barad\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of Transit Oriented Development Policies in Washington DC\n\n\n\n\n\n\n\nR\n\n\nTransportation\n\n\nCensus\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nRichard Barad\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing Fundraising Data for a Poltical Campaign\n\n\n\n\n\n\n\nPolitics\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2021\n\n\nRichard Barad\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! My name is Richard Barad and I am currently pursuing a Masters Degree in Urban Spatial Analytics at the University of Pennsylvania at the Weitzman School of Design and will be graduating in May 2024. I have experience with a wide range of data analysis, data visualization, and GIS tools including ArcGIS Pro, ArcGIS Online, Q-GIS, R, Python, SQL, Microsoft Power BI, and Carto. I am currently looking for a full time job where I can apply my data and GIS skills to policy issues. My subject matter interests and areas of experience include transportation, climate change, sustainability and environmental planning, and food security.\nPrior to attending Penn, I lived in Washington DC and spent eight years working in the International Development and Humanitarian Sectors. I worked for six years at the Famine Early Warning Systems Network (FEWS NET), a project which provides early warning on food insecurity issues around the globe. At FEWS NET, I conducted data analysis using python tools, developed interactive dashboard tools using Power BI and ArcGIS Online and provided ad-hoc GIS analysis and mapping support. I also previously worked as a Product Manager at the World Resources Institute where I managed WRI’s forest atlas initiative, a suite of tools used to build national forest and land use monitoring platforms.\nBelow you will find a selection of previous professional and academic work using R, Python, and ArcGIS tools. Click the view all projects to view additional projects."
  },
  {
    "objectID": "index.html#r-projects",
    "href": "index.html#r-projects",
    "title": "About Me",
    "section": "R Projects",
    "text": "R Projects\n\n\n\n\n\n\n\nPredicting Restaurant Inspection Failures in Chicago\n\n\n\nR\n\n\nMachine Learning\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\nPredicting Demand for Bikeshare in Boston\n\n\n\nR\n\n\nTransportation\n\n\nMachine Learning\n\n\n\n\nDec 10, 2023\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Transit Oriented Development Policies in Washington DC\n\n\n\nR\n\n\nTransportation\n\n\nCensus\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#python-projects",
    "href": "index.html#python-projects",
    "title": "About Me",
    "section": "Python Projects",
    "text": "Python Projects\n\n\n\n\n\n\n\nGlobal Dashboard for Hostspot Indentification - Famine Early Warning Systems Network\n\n\n\nFood Security\n\n\nPython\n\n\nArcGIS\n\n\nPower BI\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\nMapping Infrastructure Vulnerability to Flooding in Philadelphia\n\n\n\nEnvironment\n\n\nFlooding\n\n\nPython\n\n\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\nMapping Population Vulnerability to Flooding in Philadelphia\n\n\n\nFlooding\n\n\nPython\n\n\nCensus\n\n\nEnvironment\n\n\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Historical Voter Turnout for Primary Elections in Philadelphia\n\n\n\nPolitics\n\n\nPython\n\n\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Fundraising Data for a Poltical Campaign\n\n\n\nPolitics\n\n\nPython\n\n\n\n\nSep 26, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#arcgis-projects",
    "href": "index.html#arcgis-projects",
    "title": "About Me",
    "section": "ArcGIS Projects",
    "text": "ArcGIS Projects\n\n\n\n\n\n\n\nGlobal Dashboard for Hostspot Indentification - Famine Early Warning Systems Network\n\n\n\nFood Security\n\n\nPython\n\n\nArcGIS\n\n\nPower BI\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a Wind Suitability Map for Pennsylvania\n\n\n\nArcGIS\n\n\nEnvironment\n\n\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html",
    "href": "posts/Bike_Boston/Boston_BikeShare.html",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "",
    "text": "Many cities across the United States have started using docked bike share systems. Docked bike shares are often funded using a mix of public and private funding and are present in approximately 50 different US cities. Docked bike systems provide an additional method of commuting for workers who do not own their own bike and also provide residents with access to bikes for recreational purposes. Additionally, bikes are a clean, environmentally mode of transportation.\nOne challenge with docked bike share systems is that demand for bikes at a given location is not static over time. Demand for bikes at a given location will depend on many factors including the day of the week, time of day, the weather, and your location in the city. For example, demand for bikes at docking stations near downtown areas or universities is likely going to peak during the afternoon rush hour between the hours of approximately four to seven PM. This analysis uses Machine Learning techniques to predict demand for bikes at existing bike stations in the Boston metro area. The model predicts the number of bike trips at a given station within a one hour period.\nThe results of the model can be used by the Boston blue bikes network to help with re-balancing and redistribution of bikes to areas where there will likely be an increase in demand. The model is designed to predict demand up to 24 hours from the current time. The Boston blue bikes team can use this model to forecast demand during peak periods and then send out mini-vans to relocate bikes to areas where there is predicted increase in demand.\nThis analysis includes all stations in the Boston Bluebikes network except for bike stations located in Salem and the bike station located at the Blue Cross Blue shield office in Hingham. These stations are excluded because they are located very far away from the other stations in Bluebikes network.\nThe model presented here is designed to predict demand over summer months, and was trained on bike trip data covering a five week period from June 25th 2023 to July 31st 2023. It is important to note that this model should only be used to predict demand in the summer months due to the training data used. The model is not generalizable to the winter months when bike share demand is likely lower due to the cold weather."
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#bike-data",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#bike-data",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Bike Data",
    "text": "Bike Data\nThe first step in the analysis is to import the bike share data from BlueBikes. I import data covering June and July of 2023.\n\n\nCode\nB_Jun2023 &lt;- read.csv('C:/Users/richa/GitHub/musa_5080_HW/HW6_Bike_predictions/Data/202306-bluebikes-tripdata.csv')\nB_Jul2023 &lt;- read.csv('C:/Users/richa/GitHub/musa_5080_HW/HW6_Bike_predictions/Data/202307-bluebikes-tripdata.csv')\n\nData &lt;- rbind(B_Jun2023,B_Jul2023) %&gt;%\n  mutate(start_station_name = ifelse(start_station_name == 'Graham and Parks School – Linnaean St at Walker St','Graham and Parks School',start_station_name))\n\nbike_data &lt;- read.csv('C:/Users/richa/GitHub/musa_5080_HW/HW6_Bike_predictions/Data/current_bluebikes_stations.csv') %&gt;%\n  filter(District != 'Salem' & Name != 'BCBS Hingham') %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs='EPSG:4326') %&gt;% st_transform(2249)\n  \n\nrm(B_Jun2023)\nrm(B_Jul2023)\n\n\nI count the number of bike share rides occurring per hour by station.\n\n\nCode\nData2 &lt;- Data %&gt;%\n    mutate(interval60 = floor_date(ymd_hms(started_at), unit = \"60 mins\")) %&gt;%\n           group_by(start_station_name,interval60) %&gt;% tally()\n\n\nNext, I build a ride panel. I also set the number of trips to zero for stations which did not have any trips occurring at a given time / date.\n\n\nCode\ndf_list &lt;- list()\n\nstations &lt;- unique(Data2$start_station_name)\n\nfor (station in stations){\n  interval60 = seq(ymd_hms('2023-06-01 00.00.00'), ymd_hms('2023-08-01 00.00.00'), by='60 mins')\n  frame &lt;-data.frame(interval60,Name=station)\n  df_list &lt;- append(df_list,list(frame))}\n\nholidays = c('2023-06-19','2023-07-04')\n\nall_data &lt;- bind_rows(df_list) %&gt;%\n  left_join(.,Data2,join_by(interval60 == interval60,Name==start_station_name)) %&gt;%\n      mutate(week = week(interval60),\n             dotw = wday(interval60, label=TRUE),\n             day = floor_date(interval60, unit = \"day\"),\n             month = floor_date(interval60, unit = \"month\"),\n             sum_rides = replace_na(n,0),\n             holiday = ifelse(substr(interval60,start=1,stop=10) %in% holidays, 1,0),\n             is_weekend = ifelse(dotw %in% c('Sat','Sun'),1,0),\n             time_of_day = case_when(hour(interval60) &lt; 5 | hour(interval60) &gt;= 23 ~ \"Overnight\",\n                                 hour(interval60) &gt;= 5 & hour(interval60) &lt; 10 ~ \"AM Commute\",\n                                 hour(interval60) &gt;= 10 & hour(interval60) &lt; 15 ~ \"Mid-Day\",\n                                 hour(interval60) &gt;= 15 & hour(interval60) &lt;= 19 ~ \"PM Commute\",\n                                 hour(interval60) &gt;= 20 & hour(interval60) &lt;23 ~ \"Evening\")) %&gt;%\n  dplyr::select(-n) %&gt;%\n  dplyr::filter(month != 1690848000)\n\nrm(Data2)\nrm(df_list)\nrm(frame)\nrm(station)\nrm(stations)\nrm(interval60)\n\n\nall_data &lt;- all_data %&gt;%\n  arrange(Name,interval60) %&gt;%\n  group_by(Name) %&gt;%\n    mutate(lag24Hour = dplyr::lag(sum_rides,24),\n           lag1week = dplyr::lag(sum_rides,24 * 7),\n           clean_lag = ifelse(dotw %in% c('Mon','Sat'),lag1week,lag24Hour),\n           week = ifelse(week==31,30,week))"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#import-census-data",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#import-census-data",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Import Census Data",
    "text": "Import Census Data\nI import census data and census tract boundaries.\n\n\nCode\nBostonCensus &lt;- \n  get_acs(geography = \"tract\", \n          variables = c(\"B01003_001\", \"B19013_001\", \n                        \"B02001_002\", \"B08013_001\",\n                        \"B08012_001\", \"B08301_001\", \n                        \"B08301_010\", \"B01002_001\"), \n          year = 2021, \n          state = \"MA\", \n          geometry = TRUE, \n          county=c(\"Suffolk\",\"Norfolk\",\"Middlesex\",\"Essex\"),\n          output = \"wide\") %&gt;%\n  rename(Total_Pop =  B01003_001E,\n         Med_Inc = B19013_001E,\n         Med_Age = B01002_001E,\n         White_Pop = B02001_002E,\n         Travel_Time = B08013_001E,\n         Num_Commuters = B08012_001E,\n         Means_of_Transport = B08301_001E,\n         Total_Public_Trans = B08301_010E) %&gt;%\n  dplyr::select(Total_Pop, Med_Inc, White_Pop, Travel_Time,\n         Means_of_Transport, Total_Public_Trans,\n         Med_Age,\n         GEOID, geometry) %&gt;%\n  mutate(Percent_White = White_Pop / Total_Pop,\n         Mean_Commute_Time = Travel_Time / Total_Public_Trans,\n         Percent_Taking_Public_Trans = Total_Public_Trans / Means_of_Transport) %&gt;%\n  st_transform(2249)\n\n\nJoin census data to bike share data and also filter out bike share stations which are not part of analysis area.\n\n\nCode\nbike_data &lt;- bike_data %&gt;% st_as_sf(coords = c('Longitude','Latitude'),crs='EPSG:4326') %&gt;% st_transform(2249) %&gt;%\n  st_join(.,BostonCensus,join=st_intersects,left = TRUE) %&gt;%\n  filter(District != 'Salem' & Name != 'BCBS Hingham')"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#import-weather-data",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#import-weather-data",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Import Weather Data",
    "text": "Import Weather Data\nI import weather data for Boston using the riem R package.\n\n\nCode\nweather.Panel &lt;- \n  riem_measures(station = \"BOS\", date_start = \"2023-06-25\", date_end = \"2023-08-01\") %&gt;%\n  mutate(interval60 = floor_date(ymd_hms(valid),unit='60 mins')) %&gt;%\n  group_by(interval60) %&gt;%\n  summarize(Temperature = mean(tmpf,na.rm=T),\n            Precip = sum(p01i, na.rm=T),\n            Wind_Speed = mean(sknt, na.rm=T)) %&gt;%\n  mutate(Temperature = na_kalman(Temperature))\n\nrainy_hours &lt;- weather.Panel %&gt;%\n  dplyr::filter(Precip &gt; 0.07)\n\n## Make a time series graph\n\n\nJoin weather data to ride panel. Also join bike share location.\n\n\nCode\nall_data &lt;- all_data %&gt;%\n  left_join(.,bike_data %&gt;% dplyr::select(-Public,-Number),by='Name') %&gt;%\n  left_join(.,weather.Panel,by='interval60') %&gt;%\n  dplyr::filter(District != 'Salem') %&gt;%\n  dplyr::filter(Name != 'BCBS Hingham') %&gt;%\n  st_as_sf()"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#create-training-and-test-data",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#create-training-and-test-data",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Create Training and Test Data",
    "text": "Create Training and Test Data\nThe trip data is split into a training and testing data. The training dataset includes all trips from June 25th 2023 to July 15th 2023 and the testing datsaet includes all trips July 15th 2023 to July 31st 2023.\n\n\nCode\nall_data &lt;- dplyr::filter(all_data,week &gt;= 26)\n\ntraining &lt;- dplyr::filter(all_data, week &lt;= 28)\n\ntest &lt;- dplyr::filter(all_data, week &gt; 28)"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#weather-patterns",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#weather-patterns",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Weather Patterns",
    "text": "Weather Patterns\nWeather patterns can be a major driver of bike demand. Bike riders may be less likely to want to use a bike when it is raining or snowing. Our analysis uses weather data from Boston Logan airport to determine weather patterns in Boston. The weather station data was downloaded using the riem package, which provides access to weather station data for airports across the country. The weather station database is maintained by Iowa State University.\nNote that this study uses weather data for Boston Logan airport for all stations in the Boston metro area. This is a limitation of the model, as weather patterns are likely to vary in different parts of the city. However, using weather data for the same location helps keep the model simple and will improve its utility for making predictions once it is operationalized. When making predictions for bike share demand, officials will only need to provide the model with a rough rainfall forecast for the entire city. This can be easily obtained from weather reports. It would not be practical for BlueBikes to obtain separate forecasts for each bike station in the Boston metro area.\nThe graph below shows the Precipitation and Temperature patterns over the time period which is used for training the model. We note multiple days with large amount of rains including June 27th, June 28th, July 2nd, July 10th, July 16th, July 21st, July 25th, and July 29th. The temperature ranges between 60 and 90 degrees Fahrenheit and the average temperature is around 75 to 80 degrees. The temperature range during the months of June and July in Boston is ideal to for biking so temperature will likely not have a major impact on bike share demand during the period used for training the model.\n\n\nCode\ngrid.arrange(\n  ggplot(weather.Panel, aes(interval60,Precip)) + \n    geom_line() + \n    labs(title=\"Percipitation\", x=\"Hour\", y=\"Precipitation (inches)\") +\n    scale_x_datetime(breaks = seq(as.POSIXct(\"2023-06-25\"), as.POSIXct(\"2023-07-31\"), by=\"1 day\"),date_labels = \"%b %d\")+\n    plotTheme,\n  \n  ggplot(weather.Panel, aes(interval60,Temperature)) + geom_line() + \n    scale_x_datetime(breaks = seq(as.POSIXct(\"2023-06-25\"), as.POSIXct(\"2023-07-31\"), by=\"1 day\"),date_labels = \"%b %d\")+\n    labs(title=\"Temperature\", x=\"Hour\", y=\"Temperature (Farenheit)\") +\n    plotTheme,\n  top=\"Weather Data - Boston Logan Airport (BOS) - Jun/Jul, 2023\")\n\n\n\n\n\nCode\nrm(weather.Panel)"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#temporal-trends-in-bike-share-demand",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#temporal-trends-in-bike-share-demand",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Temporal Trends in Bike Share Demand",
    "text": "Temporal Trends in Bike Share Demand\nThe chart below shows the total number of bike share trips happening per hour across the Bluebikes network. The chart includes all trips taking place between June 25th and July 31st. The background color indicates the type of day. Weekdays have a grey background, while weekends have a yellow background. The 4th of July holiday has a red background, while days when it rained have a blue background.\nThis chart shows some key temporal trends. On weekdays, there are almost allways two clear peaks in bike share demand, one in the morning during the am rush hour period and one in the evening during the PM rush hour period. The PM rush hour peak is always higher than the PM rush hour peak. On weekends there is only one peak period. On days when it rains there is a decline in demand - this decline in demand is present both on the weekend and on weekdays. On 4th of July there is a unique trend and two peaks are present, but the final peak takes place latter in the evening than usual. One hypothesis is that the second peak may be occurring after 4th of July fireworks have finished.\n\n\nCode\nggplot(all_data %&gt;% group_by(interval60,time_of_day) %&gt;% summarise(sum_count = sum(sum_rides)))+\n  geom_vline(data=all_data %&gt;% dplyr::filter(is_weekend == 1),aes(xintercept=interval60,color='weekend'),linewidth=2,alpha=0.3)+\n  geom_vline(data=all_data %&gt;% dplyr::filter(is_weekend == 0),aes(xintercept=interval60,color='weekday'),linewidth=2,alpha=0.3)+\n  geom_vline(data=rainy_hours,aes(xintercept=interval60,color='rainy day'),alpha=0.3,linewidth=2)+\n  geom_vline(data=all_data %&gt;% dplyr::filter(holiday == 1),aes(xintercept=interval60,color='holiday'),linewidth=2,alpha=0.3)+\n  geom_line(aes(x = interval60, y = sum_count),color='#0f3f01',linewidth=0.7)+\n  scale_color_manual(values = c('#ffaaaa','#89ebff','grey90','#fcf458'),name='Event Type')+\n  labs(title=\"Bike share trips per hr. Boston, Jun-July, 2023\",\n       x=\"Hour\", \n       y=\"Number of trips\")+\n  scale_x_datetime(breaks = seq(as.POSIXct(\"2023-06-25\"), as.POSIXct(\"2023-07-31\"), by=\"1 day\"),\n               date_labels = \"%b %d\")+\n  plotTheme\n\n\n\n\n\nCode\nrm(rainy_hours)\n\n\nThe first chart below shows below the number of rides taking place per day across the BlueBike network broken down by time of day. The number of rides which occur per day is generally around 12,500 to 15,000 rides. Notably, the total number of rides declines sharply on days when heavy rain storms occurred. Examples of this include July 2nd, July 10th, July 16th, and July 29th. The total number of rides occurring on weekend days is similar to the total number of rides occurring on weekdays.\nThe second chart shows the percentage of rides taking place during different times of day. Generally, the largest percentage of rides take place during the PM commute which takes place between 3 and 7pm. The smallest percentage of rides take place overnight - the overnight period includes rides occurring between 11pm and 5am. Notably, the percentage of rides which occur during the overnight period increases on Friday and Saturday nights as residents return home after going to restaurants and bars. I hope they are not biking drunk!\n\n\nCode\ngrid.arrange(ncol=2,\n\nall_data %&gt;% group_by(day,time_of_day) %&gt;% summarise(sum_count = sum(sum_rides)) %&gt;%\n  st_drop_geometry() %&gt;%\n  ggplot(aes(x=as.character(day),y=sum_count,fill=time_of_day))+\n  geom_bar(stat='identity')+\n  scale_fill_viridis_d(direction=-1, name='Time of Day')+\n  labs(y='Number of Rides',x='Day')+\n  ggtitle(\"Number of Rides by Day and Time of Day\")+\n  plotTheme+\n  theme(axis.text.x = element_text(angle = 90,vjust=0.5)),\n\nall_data %&gt;% group_by(day,time_of_day) %&gt;% summarise(sum_count = sum(sum_rides)) %&gt;%\n  st_drop_geometry() %&gt;%\n  ggplot(aes(x=as.character(day),y=sum_count,fill=time_of_day))+\n  geom_col(position='fill',stat='identity')+\n  scale_fill_viridis_d(direction=-1,name='Time of Day')+\n  labs(y='Perecent of Total Rides',x='Day')+\n  ggtitle(\"Percent of Daily Rides by Time of Day\")+\n  plotTheme+\n  theme(axis.text.x = element_text(angle = 90,vjust=0.5))\n)\n\n\n\n\n\nThe chart below shows the total number of bike share trips occurring in Boston per hour by day of the week and by weekend/weekday. On weekdays, we can clearly see the two peaks during the PM and AM commute periods. The peak is higher during the PM commute period. On weekends, the majority of trips occur between the hours of 11am and 7pm. The number of trips tends to be lower on Friday and Tuesdays relative to other days. The lower number of trips on Friday can likely be attributed to fewer commuters going to the office to work in person on Fridays. The lower number of trips on Tuesday may be a results of 4th of July and the rain event on Tuesday July 25th deflating the total number of trips on Tuesdays.\n\n\nCode\npalette &lt;- c('#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00','#ffff33','#a65628')\n\ngrid.arrange(ncol=2,\n\nall_data %&gt;% \n  st_drop_geometry() %&gt;%\n  mutate(hour = hour(interval60)) %&gt;% \n  group_by(hour,dotw) %&gt;% summarise(sum_count = sum(sum_rides)) %&gt;%\n  ggplot()+\n  geom_line(aes(x=hour, y=sum_count,color = dotw), binwidth = 1)+\n  geom_point(aes(x=hour, y=sum_count,color = dotw),size=1)+\n  scale_color_manual(values = palette, name='Day of Week')+\n  scale_x_continuous(breaks = seq(0,23,1),labels=c('12pm','1am','2am','3am','4am','5am','6am','7am','8am','9am','10am','11am','12am','1pm','2pm','3pm','4pm','5pm','6pm','7pm','8pm','9pm','10pm','11pm'))+\n  labs(title=\"Bike share trips in Boston by day of the week, June/July, 2023\",\n       x=\"Hour\", \n       y=\"Trip Counts\")+\n     plotTheme,\n\nall_data %&gt;% \n  st_drop_geometry() %&gt;% \n  mutate(hour = hour(interval60)) %&gt;% \n  group_by(hour,is_weekend) %&gt;% summarise(sum_count = sum(sum_rides)) %&gt;%\n  ggplot()+\n  geom_line(aes(x=hour, y=sum_count,color = as.character(is_weekend)), binwidth = 1)+\n  geom_point(aes(x=hour, y=sum_count,color = as.character(is_weekend)),size=1)+\n  scale_color_manual(values=c('red','purple'),name='Weekend',labels = c('Weekday','Weekend'))+\n  scale_x_continuous(breaks = seq(0,23,1),labels=c('12pm','1am','2am','3am','4am','5am','6am','7am','8am','9am','10am','11am','12am','1pm','2pm','3pm','4pm','5pm','6pm','7pm','8pm','9pm','10pm','11pm'))+\n  labs(title=\"Bike share trips in Boston by weekend/weekday, June/July, 2023\",\n       x=\"Hour\", \n       y=\"Trip Counts\")+\n     plotTheme)"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#spatialtemporal-trends-in-bikeshare-demand",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#spatialtemporal-trends-in-bikeshare-demand",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Spatial/Temporal Trends in Bikeshare demand",
    "text": "Spatial/Temporal Trends in Bikeshare demand\nThe maps below shows the mean number of trips originating in each census by time of day on weekends and weekdays. The maps show a strong spatial clustering pattern to where trips originate. The areas where the largest number of bike trips start are located in Cambridge and downtown Boston. There are very large number of trips originating in census tracts around MIT and Harvard. Notably, the time of day does not appear to have a major impact on where trips originate from. The hotspots for trip origins appear to remain relatively constant across the different times of day. The origin hotspots are also similar on weekdays and weekends. We can again observe that the number of trips is highest on weekdays during the PM commute period when residents are returning home from work. The increase during the PM commute period is most notable in downtown Cambridge and downtown neighborhoods of Boston.\n\n\nCode\nMA_towns &lt;- st_make_valid(st_read('https://arcgisserver.digital.mass.gov/arcgisserver/rest/services/AGOL/Census2020_Towns/MapServer/2/query?where=1%3D1&f=geojson')) %&gt;% st_transform('EPSG:2249')\n\ntowns_filt &lt;- MA_towns[st_is_within_distance(st_centroid(MA_towns),bike_data,10000) %&gt;% lengths &gt; 0,] %&gt;%\n  mutate(town = str_extract(NAMELSAD20, \"\\\\w+\"))\n\ncensus_filt &lt;- BostonCensus[st_is_within_distance(st_centroid(BostonCensus),bike_data,5280) %&gt;% lengths &gt; 0,]\n\nparks &lt;- st_read('https://arcgisserver.digital.mass.gov/arcgisserver/rest/services/AGOL/OpenSpaceLevProt/MapServer/0/query?where=1%3D1&text=&objectIds=&time=&geometry=%7Bxmin%3A+-70.99%2C+ymin%3A+42.43%2C+xmax%3A+-71.18%2C+ymax%3A+42.3%7D&geometryType=esriGeometryEnvelope&inSR=4326&spatialRel=esriSpatialRelIntersects&distance=&units=esriSRUnit_Foot&relationParam=&outFields=&returnGeometry=true&returnTrueCurves=false&maxAllowableOffset=&geometryPrecision=&outSR=&havingClause=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&historicMoment=&returnDistinctValues=false&resultOffset=&resultRecordCount=&returnExtentOnly=false&datumTransformation=&parameterValues=&rangeValues=&quantizationParameters=&featureEncoding=esriDefault&f=geojson')\n\n\n\n\nCode\ndata_by_day &lt;- all_data %&gt;% \n  mutate(weekend = ifelse(is_weekend == 1, \"Weekend\",\"Weekday\")) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(day, time_of_day, GEOID, weekend) %&gt;% summarize(sum_rides = sum(sum_rides))%&gt;%\n  group_by(time_of_day, GEOID, weekend) %&gt;% summarize(mean_rides = mean(sum_rides))%&gt;%\n  inner_join(census_filt %&gt;% select(GEOID,geometry),.,by='GEOID') \n\nggplot()+\n  geom_sf(data = towns_filt,fill='grey95')+\n  geom_sf(data = data_by_day,aes(fill = cut(mean_rides, breaks = c(-1,1,5,10,50,100,1000))),color='transparent')+\n  scale_fill_viridis_d(direction = -1,name='Mean Number of Rides', labels = c('0-1','1-5','5-10','10-50','50-100','100+'))+\n  geom_sf(data = towns_filt,color='grey30',fill='transparent')+\n  geom_sf_text(data = towns_filt,aes(label=town),size=2.5,position='jitter') + \n  facet_grid(weekend ~ time_of_day)+\n  labs(title=\"Mean Number of trips by census tract and time period on weekends and weekdays. Boston, June/July, 2023\")+\n  mapTheme"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#spatial-patterns-in-trip-routes",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#spatial-patterns-in-trip-routes",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Spatial Patterns in Trip Routes",
    "text": "Spatial Patterns in Trip Routes\nThe map below looks at the most common bike trip routes. The data shown is for the week of July 11th 2023 to July 15th, all routes with more than three trips are included. The thickness and color of the line indicates the number of trips along a given route. Routes with many frequent daily trips includes rides from South Boston to Downtown and rides within Cambridge. We can also observe that with few exceptions the typical trip length with a BlueBike is not long in distance. This map, along with the previous map also reveal that there are many bike stations where usage is very low.\n\n\nCode\ntowns_filt2 &lt;- towns_filt %&gt;% st_transform('EPSG:4326') %&gt;% st_crop(xmin= -70.99, xmax=-71.18, ymin=42.43,ymax=42.3) %&gt;% st_transform('EPSG:2249') \n\nparks &lt;- parks %&gt;% st_transform('EPSG:2249') %&gt;% st_intersection(.,towns_filt2)\n\nbikes_filt2 &lt;- bike_data[st_intersects(bike_data,towns_filt2) %&gt;% lengths &gt; 0,] \n\nbbox = st_bbox(towns_filt2)\n\ntowns_no_water &lt;- towns_filt2 %&gt;% erase_water()\n\nplay &lt;- Data %&gt;%\n  mutate(day = as.character(floor_date(ymd_hms(started_at), unit = \"day\"))) %&gt;%\n  filter(day == '2023-07-10'| day == '2023-07-11' | day == '2023-07-12' | day == '2023-07-13' | day == '2023-07-14' | day == '2023-07-15') %&gt;%\n  group_by(start_station_name, end_station_name) %&gt;%\n  tally() %&gt;%\n  ungroup() %&gt;%\n  filter(n &gt; 3) %&gt;%\n  filter(start_station_name != end_station_name) %&gt;%\n  left_join(bike_data %&gt;% dplyr::select(Name, geometry), by = c(\"start_station_name\" = \"Name\")) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(start_lat = st_coordinates(geometry)[,2],\n         start_long = st_coordinates(geometry)[,1]) %&gt;%\n  st_drop_geometry() %&gt;%\n  left_join(bike_data %&gt;% dplyr::select(Name, geometry), by = c(\"end_station_name\" = \"Name\")) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(end_lat = st_coordinates(geometry)[,2],\n         end_long = st_coordinates(geometry)[,1]) %&gt;%\n  st_drop_geometry() %&gt;%\n  arrange(n)\n\nggplot()+\n  geom_rect(aes(xmin= bbox$xmin, xmax= bbox$xmax, ymin= bbox$ymin, ymax = bbox$ymax),fill='#c2d4fc')+\n  geom_sf(data=towns_no_water,color=\"transparent\",fill='grey70')+\n  geom_sf(data=parks,fill='#c2fcd5',color='transparent',alpha=0.5)+\n  geom_sf(data=towns_filt2,fill=\"transparent\",color='grey30')+\n  geom_segment(data=play,aes(x=start_long,y=start_lat,xend=end_long,yend=end_lat,alpha=n,color=n,linewidth=n))+\n  scale_color_viridis(direction=-1)+\n  labs(color='Number of Trips',fill='')+\n  geom_sf(data=bikes_filt2,size=1,color='lightyellow')+\n  geom_sf_text(data = towns_filt2,aes(label=town),size=3.5,position='jitter',color='white')+\n  scale_alpha(range = c(0.2,1),guide = \"none\")+\n  scale_linewidth(range = c(0.4,1),guide = \"none\")+\n  scale_x_continuous(limits = c(bbox$xmin, bbox$xmax)) +\n  scale_y_continuous(limits = c(bbox$ymin, bbox$ymax))+\n  theme_void()+\n  theme(legend.position = c(0.85, 0.15), \n        legend.background = element_rect(fill=\"transparent\", colour =\"transparent\"))"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#temporal-lags",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#temporal-lags",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Temporal Lags",
    "text": "Temporal Lags\nOn method of improving the model is to use temporal lags. Temporal lags are based on the concept that what happened in the recent past is likely to happen again in the future. Bike share patterns are likely to cluster in time, and we can take advantage of this pattern to build a more predictive model. Because our goal is to build a model which can be used with a 24 hour leeway, we can only use spatial lags which are more than or equal to 24 hours. As seen by previous exploratory analysis, the number of bike trips occurring at a given hour are likely to be similar to the number of trips which occurred at the same hour on the previous day, and at the same hour and day on the previous week. For this reason, we decide to develop a 24 hour temporal lag which represents the number of bike trips at a station at the same time on the previous day, and a 1 week lag which represents the number of bike trips which occurred at the same hour one week ago.\nOne limitation with using the 24 hour lag is that bike trends on Monday and Saturday are not likely to be similar to the trends which occurred on the previous day. For this reason, we develop a third lag called the “clean-lag”. The clean lag uses the 24 hour temporal lag on all days except for on Mondays and Saturdays when the 7 day lag is used.\nThe charts below show the correlation between the 24 hour lag, the 1 week lag, and the clean lag and the number of bike trips at a given hour. As shown, there is generally a positive correlation between the number number of bike trips occuring at a station at a given hour and the temporal lag. The charts are also show the Pearson correlation value (i.e: R) which measures the strength of linear relationship between two variables. As shown, the clean lag has the highest correlation value, followed by the 1 week temporal lag.\n\n\nCode\ntraining %&gt;%\n  dplyr::select(lag24Hour, lag1week, clean_lag, sum_rides) %&gt;%\n  st_drop_geometry() %&gt;%\n  gather(Variable, Value, -sum_rides) %&gt;%\n  ggplot(aes(x=sum_rides, y=Value))+\n    geom_point(size=0.5)+\n    geom_smooth(method=\"lm\", se=FALSE, fullrange=FALSE)+\n    stat_cor(method= \"pearson\")+\n    facet_wrap(~Variable)+\n    labs(x=\"Number of Bike Trips\",y=\"Temporal Lag - Number of Trips\")+\n    plotTheme"
  },
  {
    "objectID": "posts/Bike_Boston/Boston_BikeShare.html#assessing-model-errors",
    "href": "posts/Bike_Boston/Boston_BikeShare.html#assessing-model-errors",
    "title": "Predicting Demand for Bikeshare in Boston",
    "section": "Assessing Model Errors",
    "text": "Assessing Model Errors\nHaving trained each of the modelling, we can now move on to testing our model against unseen data. The models were trained on data covering June 25th 2023 to July 15th 2023 and tested on data covering July 15th 2023 to July 31st 2023.\n\n\nCode\nride.Test.weekNest &lt;- \n  test %&gt;%\n  nest(-week) \n\n\n\n\nCode\nmodel_pred &lt;- function(dat, fit){\n   pred &lt;- predict(fit, newdata = dat)}\n\n\n\n\nCode\nweek_predictions &lt;- \n  ride.Test.weekNest %&gt;% \n    mutate(ATime_FE = map(.x = data, fit = reg1, .f = model_pred),\n           BTime_Space_FE = map(.x = data, fit = reg2, .f = model_pred),\n           CTime_Space_Weather = map(.x = data, fit = reg3, .f = model_pred),\n           DTime_Space_Weather_timeLags = map(.x = data, fit = reg4, .f = model_pred),\n           ETime_Space_Wather_timeLags_notemp = map(.x = data, fit = reg5, .f = model_pred)) %&gt;% \n    gather(Regression, Prediction, -data, -week) %&gt;%\n    mutate(Observed = map(data, pull, sum_rides),\n           Absolute_Error = map2(Observed, Prediction,  ~ abs(.x - .y)),\n           MAE = map_dbl(Absolute_Error, mean, na.rm = TRUE),\n           sd_AE = map_dbl(Absolute_Error, sd, na.rm = TRUE))\n\n\nThe next step is to assess the performance of our model. I use the five models to predict what ride share demand will be for the data points in our testing dataset. The testing dataset covers the last two weeks of July, and each model is used to predict the number of trips per hour at all stations within the study. The graph below shows the Mean absolute error by week for the five models. As shown below, model E has the lowest error in both week 3 and week 4 and appears to have the highest accuracy of the five models.\n\n\nCode\nweek_predictions %&gt;%\n  dplyr::select(week, Regression, MAE) %&gt;%\n  gather(Variable, MAE, -Regression, -week) %&gt;%\n  ggplot(aes(as.character(week), MAE)) + \n    geom_bar(aes(fill = Regression), position = \"dodge\", stat=\"identity\") +\n    scale_fill_viridis_d(option='rocket',direction=-1) +\n    scale_x_discrete(labels=c(\"Jul. 2023 Week 3\",\"Jul. 2023 Week 4\"))+\n    labs(title = \"Mean Absolute Errors by model specification and week\",x='Week',y='Mean Absoulte Error (MAE)') +\n    plotTheme\n\n\n\n\n\nThe charts below show the total number of bike trips per hour predicted by our five models and compares the prediction to actual observations. The predictions are shown by the blue line while the observed line represents actual data. The model in which the prediction line most closely follows the observed line is model E, reinforcing the conclusion that model E has the best performance. Generally speaking, the prediction line for model E closely follows the observed line. Exceptions are present on July 16th, July 17th, and July 25th. Looking back at our exploratory analysis, can revel some clues as to why the model performs poorly on these dates.\n\nJuly 16th was a very rainy day, the rainfall charts shows that the duration of the rainfall was very high but the intensity was low. The current model is based only on rainfall intensity and not duration. The model may be improved by adding a variable for rainfall duration.\nJuly 17th occurred one week after July 10th and July 10th was a very rainy day. As a result, the number of trips indicated by the 1 week temporal lag is likely not an accurate representation of bike share demand on July 27th.\nOn July 25th a rainfall event occurred during the commute period. Despite include rainfall as a predictor, the predictions produced by the model are not adequately accounting for rainfall.\n\nWhile model E does have limitations, it does appear to produce close to accurate predictions on most days and the average error is around one bike trip. This indicates the model accuracy is high, despite having some challenges with effectively capturing the impacts of weather anomalies.\n\n\nCode\nweek_predictions %&gt;% \n    mutate(interval60 = map(data, pull, interval60),\n           Name = map(data, pull, Name)) %&gt;%\n    dplyr::select(interval60, Name, Observed, Prediction, Regression) %&gt;%\n    unnest() %&gt;%\n    gather(Variable, Value, -Regression, -interval60, -Name) %&gt;%\n    group_by(Regression, Variable, interval60) %&gt;%\n    summarize(Value = sum(Value)) %&gt;%\n    ggplot(aes(interval60, Value, colour=Variable)) + \n      geom_line(size = 1.1) + \n      facet_wrap(~Regression, ncol=1) +\n      labs(title = \"Predicted/Observed bike share time series\", subtitle = \"Boston; A test set of 2 weeks\",  x = \"Hour\", y= \"Station Trips\") +\n      plotTheme+\n  scale_x_datetime(breaks = seq(as.POSIXct(\"2023-07-16\"), as.POSIXct(\"2023-07-31\"), by=\"1 day\"),\n               date_labels = \"%b %d\")"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "",
    "text": "This project was prepared as a final project for the MUSA-508: Public Policy Analytics at the University of Pennsylvania. The project uses machine learning techniques, specifically logistic regression to build a model to forecast the likelihood of restaurants in the city of Chicago passing their health inspections. The analysis relies heavily on data from the Chicago Open Data Portal and data from the U.S Census, American Community Survey dataset.\nFor an abbreviated high level summary of this analysis please watch our video which is viewable below. Continuing reading for additional information on the project context, motivation, exploratory data analysis, methodology, and results."
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#import-inspection-data",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#import-inspection-data",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Import Inspection Data",
    "text": "Import Inspection Data\nWe begin building our predictive model by importing food inspection data from the city of Chicago, covering all records since our analysis focuses on historical failures. The provided information originates from assessments conducted on restaurants and food establishments in Chicago from January 1, 2010 to present. These inspections are carried out by personnel from the Chicago Department of Public Health’s Food Protection Program. Subsequently, the results are entered into a database for approval by a State of Illinois Licensed Environmental Health Practitioner. After importing the data on food inspections, we clean and standardize the business names along with the facility types. Our selectiveness ensures the predictive model will be built on the types of establishments (i.e: Restaurants, Bakeries, and Coffee Shops) we are trying to target for the Local Restaurant Improvement Fund.\n\n\nCode\n#Read data from city of Chicago on inspections - we need all data since we look at historical failures\n\ndata &lt;- read.socrata(\"https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5\") %&gt;%\n  na.omit() %&gt;%\n  mutate(inspection_date = ymd(inspection_date),\n         year = year(inspection_date),\n         month = month(inspection_date),\n         address = str_squish(address),\n         facility_type = str_squish(str_to_title(facility_type)))\n\n#Clean up a bunch of messy facility names to be able to identify just facilities of (i.e: restaurants, cafes,and bakeries)\n\ndata$facility_type[grepl(\"Bakery\", data$facility_type)] &lt;- \"Bakery\"\ndata$facility_type[grepl(\"Coffee\", data$facility_type)] &lt;- \"Coffee Shop\"\ndata$facility_type[grepl(\"Ice Cream\", data$facility_type)] &lt;- \"Ice Cream Shop\"\ndata$facility_type[grepl(\"Deli\", data$facility_type)] &lt;- \"Deli\"\ndata$facility_type[grepl(\"Taqueria\", data$facility_type)] &lt;- \"Restaurant\"\ndata$facility_type[grepl(\"Hot Dog Station\", data$facility_type)] &lt;- \"Restaurant\"\ndata$facility_type[grepl(\"Juice and Salad Bar\", data$facility_type)] &lt;- \"Restaurant\"\ndata$facility_type[grepl(\"Restaurant\", data$facility_type)] &lt;- \"Restaurant\"\n\n#Standardize some names of businesses\n\ndata$dba_name[grepl(\"SEE THRU CHINESE\", data$dba_name)] &lt;- 'SEE THRU CHINESE RESTAURANT'\ndata$dba_name[grepl(\"JAMAICAN  GATES\", data$dba_name)] &lt;- 'JAMAICAN GATES'"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#import-business-license-data",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#import-business-license-data",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Import Business License Data",
    "text": "Import Business License Data\nWe download business licenses from the Chicago Open Data Portal. These licenses are issued by the Department of Business Affairs and Consumer Protection in the City of Chicago, covering the period from 2002 to the present. Next, we proceed with importing and cleaning the data on business licenses. The data retrieval encompasses all licenses issued since 2010, the year our analysis starts given the limitation that our inspection data does not extend beyond the last 10 years. The business license data will be used to estimate the age of the restaurant.\n\n\nCode\n# Download Business Licenses From the Chicago Data Portal\n\nliscenses &lt;- read.socrata(\"https://data.cityofchicago.org/resource/r5kz-chrr.json?$where=license_start_date%20between%20%272010-01-01T12:00:00%27%20and%20%272021-12-31T14:00:00%27\")\n\n\n\n\nCode\n# Manual Cleaning Of Addresses\n\ncorrections &lt;- data.frame(\n  bad = c(\"4623-4627 N BROADWAY  1 & 2\",\"100 E WALTON ST 1 104\",\"436-440 E 79TH ST\",\"1733 W 87TH ST 1ST FLOOR\",\"163 E WALTON ST 2ND F\",\"5640 S UNIVERSITY AVE\",\"5255 W MADISON ST 1 B\",\"111 E 51ST ST 1ST`\"),\n  good = c(\"4623-4627 N BROADWAY\", \"100 E WALTON ST\",\"436 - 440 E 79TH ST\",\"1733 W 87TH ST\",\"163 E WALTON ST\",\"5640 S UNIVERSITY\",\"5255 W MADISON ST\",\"111 E 51ST ST 1ST\"),\n  stringsAsFactors = FALSE\n)\n\nliscenses$address &lt;- match_vec(x=liscenses$address,corrections,from=1,to=2, quiet=TRUE)\n\nliscenses$address[grepl(\"47 W POLK ST\", liscenses$address)] &lt;- \"47 W POLK ST\"\n\nliscenses$address &lt;- gsub(\"\\\\s*\\\\d+$\", \"\", liscenses$address) #Remove any trailing numbers\n\nliscenses$address &lt;- gsub(\"\\\\s*\\\\d+(ST)?$\", \"\", liscenses$address) #Remove any trailing numbers which are followed by ST\n\nliscenses$address &lt;- gsub(\"\\\\s*\\\\d+(st)?$\", \"\", liscenses$address) #Remove any trailing numbers which are followed by st\n\n#Standardize some business names\n\nliscenses$doing_business_as_name[grepl(\"SEE THRU CHINESE\", liscenses$doing_business_as_name)] &lt;- 'SEE THRU CHINESE RESTAURANT'\nliscenses$doing_business_as_name[grepl(\"THE NEW VALOIS REST\", liscenses$doing_business_as_name)] &lt;- 'THE NEW VALOIS REST INC'\nliscenses$doing_business_as_name[grepl(\"CHICAGO MARRIOTT DOWNTOWN\", liscenses$doing_business_as_name)] &lt;- 'CHICAGO DOWNTOWN MARRIOTT'\nliscenses$doing_business_as_name[grepl(\"STAR OF SIAM\", liscenses$doing_business_as_name)] &lt;- 'STAR OF SIAM'\nliscenses$doing_business_as_name[grepl(\"EL CHILE RESTAURANT & PANCAKE HOUSE.\", liscenses$doing_business_as_name)] &lt;- 'EL CHILE RESTAURANT & PANCAKE HOUSE'\nliscenses$doing_business_as_name[grepl(\"FRANCES' DELI & BRUNCHERY\", liscenses$doing_business_as_name)] &lt;- \"FRANCES' REST & BRUNCHERY\""
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#read-boundary-datasets",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#read-boundary-datasets",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Read Boundary Datasets",
    "text": "Read Boundary Datasets\nWe collect the spatial data for the neighborhoods and the overall city boundary of Chicago, preparing for our spatial analysis and upcoming visualizations. The boundaries of both the city and its neighborhoods are obtained from the Chicago Data Portal. The neighborhood boundaries layer is refined to include only the neighborhoods and geometry columns, and both the neighborhoods and the city boundary are transformed to a common spatial reference system for compatibility with the other datasets that will be used in our analysis.\n\n\nCode\nneighboorhoods &lt;- st_read('C:/Users/richa/GitHub/Musa_5080_final/Data/neighboorhoods.shp')%&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(pri_neigh)\n\nchicagoboundary &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/ewy2-6yfk?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271')"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#census-data",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#census-data",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Census Data",
    "text": "Census Data\nWe download census by block group for Chicago from the 2021 American Community Survey dataset for the following variables:\n\nTotal Households with Income below poverty line\nTotal Households\nTotal Population\nTotal White Population\nMedian Rent (USD)\n\nThe dataset is cleaned and the geometries are transformed into the appropriate projected coordinate system. We use the household variables to calculate the percent of households in a census block that are below the poverty line. The percent poverty and median rent variables will be included as predictors in our model. Additionally, we calculate the percent of the population in a census tract that is white. This information is not included in our model, but will be used latter on to examine if model accuracy varies between majority white and majority non-white neighborhoods.\n\n\nCode\nvariables = c(\"B17017_002\", #Total Households w/ income below poverty line\n              \"B17017_001\", #Total Households \n              \"B02001_001\", #Total Population\n               \"B02001_002\", #Total White Population\n               \"B25058_001\") #Median Rent\n\ncensus_data &lt;- get_acs(\"block group\",\n        year=2021,\n        output='wide',\n        geometry=T,\n        variables = variables,\n        state = 'IL',\n        county = 'Cook'\n        ) %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  select(ends_with('E'),'GEOID') %&gt;%\n  rename(poverty = \"B17017_001E\",\n         below_poverty_line = \"B17017_002E\",\n         Total_Population = \"B02001_001E\",\n         White_Population = \"B02001_002E\",\n         Median_Rent = \"B25058_001E\") %&gt;%\n  mutate(Median_Rent = ifelse(is.na(Median_Rent),median(Median_Rent,na.rm=TRUE),Median_Rent),\n         pct_non_white = 100 - ifelse(Total_Population == 0,0,White_Population / Total_Population * 100),\n         pct_poverty = ifelse(poverty == 0,0, below_poverty_line / poverty * 100)) %&gt;%\n  select('Median_Rent','pct_non_white','pct_poverty','GEOID')"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#clean-inspections-dataset",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#clean-inspections-dataset",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Clean Inspections Dataset",
    "text": "Clean Inspections Dataset\nThe first step in building the dataset we use to train our model involves filtering and cleaning the inspection data. This process involves projecting the restaurant data into the appropriate coordinate system and selecting just facilities that match the types of businesses eligible for funding through the restaurant improvement program. We also exclude the inspections conducted in the boundary housing the Chicago O’Hare International Airport and inspections of restaurants located inside Midway International Airport. We treat instances labeled as “Pass w/ Conditions” as equivalent to a regular “Pass,” and conduct a spatial join between the restaurants and neighborhood layer.\nWe build two clean datasets, one for 2021 and another for 2020. The 2021 will be used for building our model while the 2020 dataset will be used to engineer features about restaurant inspection pass rates in the previous year.\n\n\nCode\nfilter &lt;- c(\"Restaurant\",\"Bakery\",\"Tavern\",\"Ice Cream Shop\",\"Deli\",\"Cafe\",\"Coffee Shop\",\"\")\n\n#Function to clean inspection data, join to neighboorhoods and filter to just the year of interest\n\nclean_inspections &lt;- function(df,y){\n  clean_df &lt;- df %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326, agr = \"constant\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::filter(year == y & results != 'Out of Business' & results != 'No Entry' & results != 'Not Ready' & results != 'Business Not Located') %&gt;%\n  dplyr::filter(facility_type %in% filter) %&gt;%\n  mutate(results = ifelse(results==\"Pass w/ Conditions\",\"Pass\",results),\n         fail = ifelse(results==\"Fail\",1,0)) %&gt;%\n  st_join(.,neighboorhoods,predicate = st_intersects) %&gt;%\n  mutate(pri_neigh = ifelse(location == \"(42.008536400868735, -87.91442843927047)\",\"O'Hare\",pri_neigh),\n         pri_neigh = ifelse(location %in% c(\"(41.892249163400116, -87.60951804879336)\",\"(41.89233780863412, -87.6040447589981)\"),\"Streeterville\",pri_neigh)) %&gt;%\n  dplyr::filter(pri_neigh != \"O'Hare\") %&gt;% # Remove Ohare Restaurants\n  dplyr::filter(address != '5700 S CICERO AVE') # Remove Midway Restaurants\n  return(clean_df)\n}\n\ndata_2021 &lt;- clean_inspections(data,2021)\n\ndata_2020 &lt;- clean_inspections(data,2020)"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#determine-restaurant-age",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#determine-restaurant-age",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Determine Restaurant Age",
    "text": "Determine Restaurant Age\nThis code calculates the age of each business by referencing the date of its initial business license application. Specifically relating to retail establishments, it identifies the date when the license was first obtained. The minimum date is then joined back to the original license data and a subset of relevant columns is selected. The resulting age is estimated in days with a maximum age expected to be around 4,380 days (i.e: 365 days x 12 years).\nTo enhance accuracy, our code attempts various join methods to join the business license to the inspection data, as direct joining on the license id did not consistently yield matches. The time frame considered spans 12 years, reflecting the available license data from 2010 onward. This ensures that the analysis focuses on the relevant period up to the year 2021. For roughly 6% of 2021 inspections it was not possible to determine the age of the restaurant - we set the age of these restaurants equal to the city wide median. The map below shows the results of our age analysis for restaurants included in the 2021 inspection dataset.\n\n\nCode\n#For each for retail establishment determine date when a license was first obtained\nliscense_min &lt;- liscenses %&gt;% group_by(doing_business_as_name,address) %&gt;% summarize(min_date = min(license_start_date)) %&gt;%\n  arrange(min_date)\n\n#Join min date back to original licence data and select subset of columns \n\nliscenses_final &lt;- left_join(liscenses, liscense_min , by = c('doing_business_as_name','address')) %&gt;%\n  select(license_id, account_number,legal_name,doing_business_as_name,address,site_number,min_date) %&gt;%\n  mutate(license_id = as.integer(license_id))\n\n#Join date business applied for first licence to 2021 inspection data - try to joins a variety of different ways since joining on licence number did not match allways\n\ndata_2021_2 &lt;- left_join(data_2021,liscenses_final %&gt;% select(license_id,min_date),by=join_by(license_==license_id)) %&gt;%\n  rename(min_date1 = 'min_date') %&gt;%\n  left_join(.,liscense_min %&gt;% select(doing_business_as_name,address,min_date),by=join_by(dba_name==doing_business_as_name,address==address),multiple='first') %&gt;%\n  rename(min_date2 = 'min_date') %&gt;%\n  left_join(.,liscense_min %&gt;% select(doing_business_as_name,address,min_date),by=join_by(aka_name==doing_business_as_name,address==address),multiple='first') %&gt;%\n  rename(min_date3 = 'min_date') %&gt;%\n  mutate(min_date = pmin(min_date1, min_date2, min_date3, na.rm = TRUE), #Select lowest date where multiple joins worked\n         age = as.integer(difftime(inspection_date, min_date, units = 'days'))) %&gt;%\n  select(-min_date1,-min_date2,-min_date3)\n\ndata_2021_2 &lt;- data_2021_2 %&gt;%\n  mutate(age = ifelse(age&lt;0,0,age),\n         age = ifelse(is.na(age),median(age,na.rm=TRUE),age))\n\nggplot() +\n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = data_2021_2, aes(color = age), size = 0.5) +\n  scale_color_viridis_c(name = \"Age (Days)\") +\n  labs(title = \"Restaurant Age (Days)\") +\n  theme_void()"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#estimate-number-of-previous-violations",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#estimate-number-of-previous-violations",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Estimate Number of Previous Violations",
    "text": "Estimate Number of Previous Violations\nThis code estimates the number of previous violations for each business within the dataset. It begins by filtering the data based on the condition that the year is earlier than 2021 and the outcome is defined as ‘Fail’. We then group the dataset by business name and address, and the violations are counted. The results are then joined to the 2021 inspection data after being renamed. The purpose of this code is to provide an overview of the historical violation records associated with each business, helping us understand the full compliance history throughout the city of Chicago.\nWe also use the age variable and the previous fail variable to calculate the number of failures per day open. This variable serves as an interaction term that considers both the age of the restaurant and the previous number of failures. The map below show the results of our previous inspection failure analysis for all restaurants in the 2021 inspection dataset.\n\n\nCode\nfails &lt;- data %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::filter(year&lt;2021 & results == 'Fail') %&gt;%\n  group_by(dba_name,address) %&gt;% tally() %&gt;%\n  ungroup() %&gt;%\n  rename(prev_fails = 'n')\n\ndata_2021_2 &lt;- left_join(data_2021_2,fails,by=join_by(dba_name==dba_name,address==address)) %&gt;%\n  mutate(prev_fails = replace_na(prev_fails,0),\n         fails_Per_day = replace_na(prev_fails/age,0),\n         fails_Per_day =  ifelse(is.infinite(fails_Per_day), prev_fails, fails_Per_day))\n\nggplot() +\n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = data_2021_2, aes(color = cut(prev_fails, breaks = c(-1, 0, 1, 2, 5, max(prev_fails, na.rm=TRUE)))), size = 0.3)+\n  scale_color_viridis_d(name = \"# of Failures\", labels = c('0','1','2','3-5', '&gt;5'))+\n  labs(title = \"Number of Previous Inspection Failures\") +\n  theme_void()"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#join-census-data",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#join-census-data",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Join Census Data",
    "text": "Join Census Data\nHere, we integrate census data into the 2021 restaurant inspection dataset through a spatial join, associating each restaurant record with corresponding census block group information. A neighborhood level calculation is performed to determine the mean values for variables such as percent poverty, median rent, and percent non-white by neighborhood. Restaurants that lack a specific census block group association are assigned the mean value for the neighborhood the restaurant is located in. In the end, each restaurant entry is supported with the census socioeconomic information, producing a more detailed picture of demographic condition of the area surrounding each restaurant in our 2021 inspection dataset which will be used to build our model.\nThe maps below shows the census information for the restaurants in the 2021 inspection dataset.\n\n\nCode\ndata_2021_2 &lt;- data_2021_2 %&gt;%\n  st_join(.,census_data,predicate = st_intersects)\n\nmeans_neigh &lt;- data_2021_2 %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(pri_neigh) %&gt;% summarize_at(vars(\"pct_poverty\",\"Median_Rent\",\"pct_non_white\"),mean,na.rm=TRUE)\n\ndata_2021_2 &lt;- left_join(data_2021_2,means_neigh,by='pri_neigh') %&gt;%\n  mutate(pct_poverty = ifelse(is.na(pct_poverty.x),pct_poverty.y,pct_poverty.x),\n         Median_Rent = ifelse(is.na(Median_Rent.x),Median_Rent.y,Median_Rent.x),\n         pct_non_white = ifelse(is.na(pct_non_white.x),pct_non_white.y,pct_non_white.x)) %&gt;%\n  select(-ends_with('.x'),-ends_with('.y'))\n\n\ngrid.arrange(ncol=2,\n  \nggplot() +\n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = data_2021_2, aes(color = pct_poverty), size = 0.3) +\n  scale_color_viridis_c(name = \"Percent Poverty\")+\n  labs(title = \"Percent of Households Below Poverty Line\") +\n  theme_void(),\n\nggplot() +\n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = data_2021_2, aes(color = Median_Rent), size = 0.3) +\n  scale_color_viridis_c(name = \"Median Rent\")+\n  labs(title = \"Median Rent in USD\") +\n  theme_void()\n)"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#datasets",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#datasets",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "311 Datasets",
    "text": "311 Datasets\nWe obtain sanitation complaint and rodent-baiting data from the City of Chicago Open Data Portal for 2020. We use data for the previous year, because future implementation of the model will require using 311 data for the previous year since data for the current year will not yet be available. Duplicates are removed from the dataset and any data points without geometries are also removed. We also project the data into the appropriate projected coordinate system for Chicago.\n\n\nCode\nsanitation &lt;- read.socrata(\"https://data.cityofchicago.org/resource/v6vf-nfxy.json?SR_SHORT_CODE='SCB'\")\nrodents &lt;- read.socrata(\"https://data.cityofchicago.org/resource/v6vf-nfxy.json?SR_SHORT_CODE='SGA'\")\n\nclean_311 &lt;- function(df){\n  clean_df &lt;- df %&gt;%\n  mutate(created_date = ymd_hms(created_date),\n         year = year(created_date)) %&gt;%\n  dplyr::filter(year == 2020 & duplicate=='FALSE') %&gt;%\n  dplyr::filter(!is.na(longitude)) %&gt;%\n  dplyr::filter(!is.na(latitude)) %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326, agr = \"constant\") %&gt;%\n  st_transform('ESRI:102271')\n  return(clean_df)\n}\n\nsanitation2 &lt;- clean_311(sanitation)\nrodents2 &lt;- clean_311(rodents)\n\n\nBelow, we have created a two-panel mapping visualization illustrating both the point distribution and density of sanitation complaints. In the left panel, a point map is generated by overlaying sanitation service request data onto the boundaries of Chicago. The sanitation points are highlighted in blue, offering a spatial representation of sanitation code complaints across the city. The right visualization is a density map that shows the distribution of sanitation code complaints using kernel density. This map utilizes a color gradient to depict varying density levels, with cooler colors indicating lower densities and warmer colors signifying higher concentrations. This dual-panel visualization displays the spatial distribution and density patterns of sanitation code complaints in Chicago, highlighting hotspots in the northern areas of Lake View and North Center, and in the southern parts featuring Greater Grand Crossing and Chatham.\n\n\nCode\ngrid.arrange(\n  ncol = 2,\n\n  ggplot() + \n    geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n    geom_sf(data = sanitation2, colour = \"#41B6E6\", size = 0.1, show.legend = \"point\") +\n    labs(title = \"Sanitation Code Complaints\") +\n    theme_void(),\n  \n  ggplot() + \n    geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n    stat_density2d(data = data.frame(st_coordinates(sanitation2)), \n                   aes(X, Y, fill = ..level.., alpha = ..level..),\n                   linewidth = 0.01, bins = 40, geom = 'polygon') +\n    scale_fill_distiller(palette = \"RdBu\") +\n    scale_alpha(range = c(0.00, 0.35), guide = FALSE) +\n    labs(title = \"Density of Sanitation Code Complaints\") +\n    theme_void() +\n    theme(legend.position = \"none\"))\n\n\n\n\n\nSimilar to mapping sanitation complaints, we again present the following dual-panel mapping visualization focusing in on rodent baiting data. The rodent baiting points are highlighted in blue, providing a visual representation of rodent-related service requests across the city. The right visualization features a density map that illustrates the distribution of rodent baiting incidents using kernel density estimation. This map employs a color gradient to convey varying density levels, with cooler hues representing lower densities and warmer tones indicating higher concentrations. The two visualizations illustrate the spatial distribution and density pattern of rodent baiting incidents in Chicago with notable hotspots identified in Logan Square, Avondale, North Center, and Lincoln Square.\n\n\nCode\ngrid.arrange(ncol=2,\nggplot() + \n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = rodents2, colour=\"#41B6E6\", size=0.05, show.legend = \"point\") +\n  labs(title= \"Rodent Baiting\")+\n  theme_void(),\n\nggplot() + \n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  stat_density2d(data = data.frame(st_coordinates(rodents2)), \n                 aes(X, Y, fill = ..level.., alpha = ..level..),\n                 linewidth = 0.01, bins = 40, geom = 'polygon') +\n  scale_fill_distiller(palette = \"RdBu\") +\n  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +\n  labs(title = \"Density of Rodent Baiting\") + \n  theme_void()+\n  theme(legend.position = \"none\"))"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#theft-data",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#theft-data",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Theft Data",
    "text": "Theft Data\nThirdly, same as with sanitation complaints and rodent baiting, we have generated a dual-panel mapping visualization showcasing both point distribution and density for reported theft events. We utilize theft data obtained from the City of Chicago. We preprocessed the data to convert the ‘date’ column to a datetime format and extract the corresponding year, and filters out records with missing longitude or latitude values. The visualization methods used for both the point map and density maps align with those used in the previous panel visualizations. Upon analysis, we see a notable concentration of reported thefts in and around the Loop area.\n\n\nCode\nthefts &lt;- read.socrata('https://data.cityofchicago.org/resource/qzdf-xmn8.json?primary_type=THEFT') %&gt;%\n  mutate(created_date = ymd_hms(date),\n         year = year(date)) %&gt;%\n  dplyr::filter(!is.na(longitude)) %&gt;%\n  dplyr::filter(!is.na(latitude)) %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326, agr = \"constant\") %&gt;%\n  st_transform('ESRI:102271')\n\ngrid.arrange(ncol=2,\n\nggplot() + \n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = thefts, colour=\"#41B6E6\", size=0.05, show.legend = \"point\") +\n  labs(title= \"Theft Events\") +\n  theme_void(),\n\nggplot() + \n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  stat_density2d(data = data.frame(st_coordinates(thefts)), \n                 aes(X, Y, fill = ..level.., alpha = ..level..),\n                 linewidth = 0.01, bins = 40, geom = 'polygon') +\n  scale_fill_distiller(palette = \"RdBu\") +\n  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +\n  labs(title = \"Density of Theft Events\") + \n  theme_void()+\n  theme(legend.position = \"none\"))"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#temporal-and-nearest-neighboor-analysis",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#temporal-and-nearest-neighboor-analysis",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Temporal and Nearest Neighboor Analysis",
    "text": "Temporal and Nearest Neighboor Analysis\nIn this code, we look at the percent of restaurants within 1km of each restaurant in the 2021 inspection dataset that failed an inspection in the year prior to our analysis (i.e: 2020). Building this variable, is based on principles of temporal lag, as we anticipate that areas that had a large percentage of restaurant failures in the previous year are likely to have a large percentage of restaurant failures again. Additionally, we also run a nearest neighbor analysis and calculate the average distance from each restaurant to the nearest 50 reported theft events, 50 sanitation complaints, and 50 rodent baiting events.\n\n\nCode\n# Producing a Nearest Neighbor Analysis Between the 2021 Inspection Data and 2020 Sanitation Complaints and 2020 Rodent Reports\n\nbuffer &lt;- st_buffer(data_2021_2,1000)\n\nrest_1000m &lt;- st_join(buffer,data_2020 %&gt;% dplyr::select('geometry','results')) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(inspection_id,results.y) %&gt;% tally() %&gt;%\n  ungroup() %&gt;% spread(key=results.y,value=n) %&gt;%\n  select(-'&lt;NA&gt;') %&gt;%\n  replace(is.na(.), 0) %&gt;%\n  mutate(prev_year_pct_fail = Fail / (Pass + Fail)) %&gt;%\n  replace(is.na(.), 0) %&gt;%\n  select(inspection_id,prev_year_pct_fail)\n\ndata_2021_2 &lt;- data_2021_2 %&gt;%\n    mutate(rodents.nn = nn_function(st_coordinates(data_2021_2),st_coordinates(rodents2),k = 50),\n           sanitation.nn = nn_function(st_coordinates(data_2021_2),st_coordinates(sanitation2),k = 50),\n          thefts.nn = nn_function(st_coordinates(data_2021_2),st_coordinates(thefts),k = 50)) %&gt;%\n  left_join(.,rest_1000m, by = 'inspection_id') %&gt;%\n  left_join(.,neigh_summ %&gt;%st_drop_geometry(), by = 'pri_neigh')\n\n\nThe map below shows the percent of restaurants within 1km of restaurants in the 2021 inspection dataset that failed inspection in the previous year. Restaurants in the area around the loop have a low percent failure rate. Restaurants in Auburn Gresham and Northwest Humbolt Park, and East Side have some of the highest percent failure rates for neighboring restaurants.\n\n\nCode\nggplot() +\n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = data_2021_2, aes(color = prev_year_pct_fail * 100), size = 0.3) +\n  scale_color_viridis_c(name = \"Percent Failure\") +\n  labs(title = \"Percent of Restaurants Within 1km That Failed in 2020\") +\n  theme_void()\n\n\n\n\n\nThe maps below show the results of our k nearest neighbor analysis, the average distance from restaurants to the nearest 50 rodent baiting events, sanitation complaints, and theft events. Distance is calculated in meters. A log transformation is applied to the color gradient to account for the non-normal distribution of the data.\n\n\nCode\nmy_breaks = c(50,100, 250, 500, 1000, 2000)\n\ngrid.arrange(ncol=3,\n  \nggplot() +\n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = data_2021_2, aes(color = rodents.nn), size = 0.3) +\n  scale_color_viridis_c(name = \"distance (m)\", trans = \"log\",breaks = my_breaks, labels = my_breaks)+\n  labs(title = \"Rodent Complaint\") +\n  theme_void(),\n\nggplot() +\n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = data_2021_2, aes(color = sanitation.nn), size = 0.3) +\n  scale_color_viridis_c(name = \"distance (m)\", trans = \"log\",breaks = my_breaks, labels = my_breaks)+\n  labs(title = \"Sanitation Complaint\") +\n  theme_void(),\n\nggplot() +\n  geom_sf(data = neighboorhoods, color = \"white\", fill = \"grey80\") +\n  geom_sf(data = data_2021_2, aes(color = thefts.nn), size = 0.3) +\n  scale_color_viridis_c(name = \"distance (m)\", trans = \"log\",breaks = my_breaks, labels = my_breaks)+\n  labs(title = \"Thefts\") +\n  theme_void())"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#multicollinearity-checks",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#multicollinearity-checks",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Multicollinearity checks",
    "text": "Multicollinearity checks\nNext, we examine the correlation matrix for the predictors in our model. Checking the correlation matrix is an important step because it allows us to determine if there is any severe multicollinearity between any of predictors. A logistic regression should not include variables that exhibit multicollinearity. We consider variables to be severely multicollinear if the r value is greater than 0.7 or less than -0.7. Based on the correlation matrix below we can conclude that is no severe multicollinerity between our predictors and they can all be safely included in our predictive model.\n\n\nCode\ndata_2021_2 %&gt;% dplyr::select(quant_predictors) %&gt;%\n  st_drop_geometry() %&gt;%\n  correlate() %&gt;% \n  autoplot() +\n  geom_text(aes(label = round(r,digits=2)),size = 3)"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#split-into-training-and-test-data",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#split-into-training-and-test-data",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Split into Training and Test Data",
    "text": "Split into Training and Test Data\nThe dataset containing restaurants inspection results for 2021 and our feature engineered predictors is split into two parts. Seventy percent of the data is put into the training dataset which will be used to train the logistic regression models. Thirty percent of the data is placed in the test dataset. We will predict the inspection result for each of the data points in the test dataset and compare the predicted outcome to the actual outcomes to assess the model accuracy.\n\n\nCode\n# Split Data Into 70:30 Training Dataset and Test Dataset\n\n# Taken From Housing Subsidy Assignment, 6.3.1 In Book\n\nset.seed(3456)\ntrainIndex &lt;- createDataPartition(y = paste(data_2021_2$pri_neigh, data_2021_2$fail), p = .70,list = FALSE,times = 1)\ntrain &lt;- data_2021_2[ trainIndex,]\ntest  &lt;- data_2021_2[-trainIndex,]"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#build-models",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#build-models",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Build Models",
    "text": "Build Models\n\nModel 1\nOur first model includes the following predictors: Age of restaurant, previous number of fails, fails per day open, Median Rent of census block, percent poverty of census block, and the theft, rodent, and sanitation nearest neighbor variables. The table below shows the Beta Coefficient and p-value for each of the predictors in the model. Examining the p-value can help provide a sense of the predictive power of the variable. We decide to retain all variables because all variables have a p-value that is lower than 0.3. While some of the p-values are not statistically significant they do appear to be adding some predictive power to the model.\n\n\nCode\nmodel1 &lt;- glm(fail ~ age + prev_fails + Median_Rent + rodents.nn + sanitation.nn + thefts.nn + pct_poverty + fails_Per_day,\n                     data = train,\n                     family=\"binomial\" (link=\"logit\"))\n\nsummary(model1)$coefficients[, c('Estimate','Pr(&gt;|z|)')] %&gt;%\n  kbl(col.names = c('Beta Coefficient','p-value')) %&gt;%\n  kable_minimal()\n\n\n\n\n\n\nBeta Coefficient\np-value\n\n\n\n\n(Intercept)\n-0.7656847\n0.0000007\n\n\nage\n-0.0000672\n0.0058016\n\n\nprev_fails\n0.1033694\n0.0000000\n\n\nMedian_Rent\n-0.0001739\n0.0137167\n\n\nrodents.nn\n-0.0004349\n0.0379537\n\n\nsanitation.nn\n-0.0005146\n0.0054296\n\n\nthefts.nn\n0.0006027\n0.0182976\n\n\npct_poverty\n0.0030701\n0.2333835\n\n\nfails_Per_day\n-0.2927120\n0.2303765\n\n\n\n\n\n\n\n\n\nModel 2\nIn our second model, we include all the same variables as model one. We also add the variable that indicates that percent of restaurants within 1 kilometer that failed inspections in the previous year as a predictor (i.e: prev_year_pct_fail). The Beta Coefficients and p-values for Model 2 are shown below. The prev_year_pct_fail has a low p-value so we decide to retain it in model 3.\n\n\nCode\nmodel2 &lt;- glm(fail ~ age + prev_fails + Median_Rent + rodents.nn + sanitation.nn + thefts.nn + pct_poverty + prev_year_pct_fail + fails_Per_day,\n                     data = train,\n                     family=\"binomial\" (link=\"logit\"))\n\nsummary(model2)$coefficients[, c('Estimate','Pr(&gt;|z|)')] %&gt;%\n  kbl(col.names = c('Beta Coefficient','p-value')) %&gt;%\n  kable_minimal()\n\n\n\n\n\n\nBeta Coefficient\np-value\n\n\n\n\n(Intercept)\n-0.5382775\n0.0061604\n\n\nage\n-0.0000699\n0.0042123\n\n\nprev_fails\n0.1059940\n0.0000000\n\n\nMedian_Rent\n-0.0002035\n0.0048764\n\n\nrodents.nn\n-0.0004561\n0.0294120\n\n\nsanitation.nn\n-0.0005858\n0.0019827\n\n\nthefts.nn\n0.0006686\n0.0095282\n\n\npct_poverty\n0.0028542\n0.2690009\n\n\nprev_year_pct_fail\n-0.6973265\n0.0615586\n\n\nfails_Per_day\n-0.2895797\n0.2333381\n\n\n\n\n\n\n\n\n\nModel 3\nModel three includes all the same predictors as model two but we also include the decile associated with the failure rate of the neighborhood the restaurant is located in. The predictive power of the decile variable is very high as noted by the high beta coefficient and the p-value which is close to 0.\n\n\nCode\nmodel3 &lt;- glm(fail ~ age + prev_fails + Median_Rent + rodents.nn + sanitation.nn + thefts.nn + pct_poverty + prev_year_pct_fail + decile + fails_Per_day,\n                     data = train,\n                     family=\"binomial\" (link=\"logit\"))\n\nsummary(model3)$coefficients[, c('Estimate','Pr(&gt;|z|)')] %&gt;%\n  kbl(col.names = c('Beta Coefficient','p-value')) %&gt;%\n  kable_minimal()\n\n\n\n\n\n\nBeta Coefficient\np-value\n\n\n\n\n(Intercept)\n-1.9194282\n0.0000000\n\n\nage\n-0.0000667\n0.0072735\n\n\nprev_fails\n0.0984688\n0.0000000\n\n\nMedian_Rent\n-0.0000156\n0.8347240\n\n\nrodents.nn\n-0.0001469\n0.4937983\n\n\nsanitation.nn\n-0.0002173\n0.2693802\n\n\nthefts.nn\n0.0004389\n0.0945497\n\n\npct_poverty\n0.0024765\n0.3451120\n\n\nprev_year_pct_fail\n-0.5245711\n0.1667829\n\n\ndecile\n0.1441965\n0.0000000\n\n\nfails_Per_day\n-0.3026123\n0.1983108"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#make-predictions",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#make-predictions",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Make Predictions",
    "text": "Make Predictions\nWe make predictions using all three logistic models. The result is a prediction using each of the models for each of the restaurants in the test dataset.\n\n\nCode\ntestProbs &lt;- data.frame(inspection_id = test$inspection_id,\n                        Outcome = as.factor(test$fail),\n                        probs1 = predict(model1, test, type= \"response\"),\n                        probs2 = predict(model2, test, type= \"response\"),\n                        probs3 = predict(model3, test, type= \"response\"))"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#roc-curves",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#roc-curves",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "ROC Curves",
    "text": "ROC Curves\nEach of the logistic regression models outputs a probability indicting the likelihood that a restaurant will pass an inspection. Probabilities are produced for each restaurant in the test dataset for each model.\nWe will use a receiver operating characteristic curve (ROC) to assess the overall fit of each model. In our ROC curves we plot the false positive fraction (i.e: percent of Passes incorrectly predicted) and True positive fraction (i.e: percent of passes correctly predicted) at fifty different thresholds. The ROC curves help show the trade offs in a logistic regression analysis. If we select a lower thresholds, the model will correctly predict more restaurant failures (i.e: the true positive fraction will be high). However, when a low threshold is used the model will also likely incorrectly predict many restaurants as failing inspections that actually would pass inspection (i.e: the false positive rate will also be high).\nAll three models are a step above selecting restaurants at random. The grey line shown below shows what an ROC curve would look like if restaurants were selected randomly for the restaurant improvement program.\nA useful way to compare the fit of the different logistic regression models is to calculate the Area Under (AUC) metric for each of the ROC curves. ROC curves that have a higher AUC have a better fit. The AUC values are visible in top left hand corner of the ROC plot. Model three has the highest AUC of the three models. Moving forward, we will only present results for Model 3.\n\n\nCode\nauc1 &lt;- round(pROC::auc(testProbs$Outcome,testProbs$probs1),3)\nauc2 &lt;- round(pROC::auc(testProbs$Outcome,testProbs$probs2),3)\nauc3 &lt;- round(pROC::auc(testProbs$Outcome,testProbs$probs3),3)\n\nggplot(testProbs) +\n  geom_roc(aes(d = as.numeric(Outcome), m = probs1, colour = \"Model 1\"),n.cuts = 50, labels = FALSE) +\n  geom_roc(aes(d = as.numeric(Outcome), m = probs2, colour = \"Model 2\"),n.cuts = 50, labels = FALSE) +\n  geom_roc(aes(d = as.numeric(Outcome), m = probs3, colour = \"Model 3\"),n.cuts = 50, labels = FALSE) +\n  annotate(\"text\", x = 0.1, y = 1, label=paste(\"Model 1 AUC: \",as.character(auc1)),color='#E4002B')+\n  annotate(\"text\", x = 0.1, y = 0.95, label=paste(\"Model 2 AUC: \",as.character(auc2)),color='#41B6E6')+\n  annotate(\"text\", x = 0.1, y = 0.90, label=paste(\"Model 3 AUC: \",as.character(auc3)),color='#21bf01')+\n  scale_color_manual(values = c(\"#E4002B\",\"#41B6E6\",\"#21bf01\"),name=\"Model\")+\n  style_roc(theme = theme_grey) +\n  geom_abline(slope = 1, intercept = 0, size = 1, color = 'grey60') +\n  labs(title = \"ROC Curves for Three Restaurant Inspection Models\")+\n  theme_bw()"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#predicted-probability-density",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#predicted-probability-density",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Predicted Probability Density",
    "text": "Predicted Probability Density\nThe density plots below show the density of predicted probabilities for restaurants in the test dataset that passed inspection and restaurants that failed inspections based on model three. The predicted probabilities of failing inspection for almost all points in the test dataset are under 0.50. This indicates that there is generally a high level of uncertainty on if a restaurant will pass inspection.\nHowever, a positive sign is that the peak of the predicted probability density curve for restaurants that fail inspection is higher than for restaurants that pass inspection. The most frequent predicted fail probability for restaurants that actually failed inspection is approximately 0.3. On the other hand, the most frequent fail probability for restaurants that passed inspection is approximately 0.22.\n\n\nCode\nggplot(testProbs, aes(x = probs3, fill = as.factor(Outcome))) + \n  geom_density() +\n  facet_wrap(~Outcome,ncol=1,labeller = as_labeller(c('0'='Pass','1'='Fail'))) +\n  scale_fill_manual(values=c(\"#41B6E6\",\"#E4002B\"),name=\"Outcome\")+\n  scale_x_continuous(limits=c(0,1))+\n  labs(x = \"Probability of Failure\", y = \"Density\",\n       title = \"Density of Predicted Probabilities for Restaurants in Test Dataset that failed and passed inspection\") +\n  theme(strip.text.x = element_text(size = 18),\n        legend.position = \"none\")+\n  theme_bw()\n\n\n\n\n\nThe table below show the median predicted fail probability for restaurants that pass inspection and restaurants that failed inspection. The median predicted fail probability for restaurants that fail inspection is higher than restaurants that passed inspection.\n\n\nCode\ntestProbs %&gt;%\n  group_by(Outcome) %&gt;% summarise(median_probs=median(probs3),mean_probs=mean(probs3)) %&gt;%\n  cbind(.,Outcome2=c('Pass','Fail')) %&gt;%\n  dplyr::select(Outcome2,median_probs) %&gt;%\n  kbl(col.names=c('Actual Outcome','Median Predicted Fail Probability')) %&gt;%\n  kable_minimal()\n\n\n\n\n\nActual Outcome\nMedian Predicted Fail Probability\n\n\n\n\nPass\n0.2151266\n\n\nFail\n0.2812214"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#threshold-selection",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#threshold-selection",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Threshold Selection",
    "text": "Threshold Selection\nA key component of using a logistic regression is selecting a threshold. Restaurants which are above this threshold are considered to have a high risk of failing inspection and would be the target of the restaurant improvement program. The table below presents three different accuracy metrics for different probability thresholds using the predictions in our test dataset.\nThe three accuracy measures are:\n\nSensitivity: The % of restaurants which are predicted to fail / The number of restaurants that actually failed\nSpecificity: The % of restaurants which are predicted to pass / The number of restaurants that actually passed\nAccuracy: The % of restaurants which are correctly predicted / the total number of restaurants\n\nThe table below also includes the number of restaurants that are predicted to fail at each threshold (i.e: the number of restaurants with a fail probability above the threshold). We can see that there are trade offs when selecting a threshold, when using a lower threshold the specificity tends to increase while the sensitivity and accuracy decline. We choose 0.3 as the optimal threshold - at this threshold we have an accuracy rate which is still well above 50% and identify 746 restaurants as at risk of failing inspection which is a reasonable number of restaurants to target for the restaurant improvement program.\n\n\nCode\ntresholds &lt;- c(0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6)\nsensitivity &lt;- c()\nspecificity &lt;- c()\naccuracy &lt;- c()\npred_fail &lt;- c()\n\nfor (t in tresholds){\n  predictions &lt;- as.factor(ifelse(testProbs$probs3 &gt; t,1,0))\n  r &lt;- caret::confusionMatrix(testProbs$Outcome,predictions,positive=\"1\")\n  sensitivity &lt;- append(sensitivity,r$byClass[1] * 100) \n  specificity &lt;- append(specificity,r$byClass[2] * 100)\n  accuracy &lt;- append(accuracy,r$overall[1] * 100)\n  pred_fail &lt;- append(pred_fail,sum(r$table[3:4]))\n}\n\ncbind(tresholds,sensitivity,specificity,accuracy,pred_fail) %&gt;%\n  as_data_frame() %&gt;%\n  mutate(across(where(is.numeric), ~ round(., 2))) %&gt;%\n  kbl(col.names = c(\"Threshold\",'Sensitivity (%)','Specificity (%)', 'Accuracy (%)', 'Predicted Failures')) %&gt;%\n  kable_paper() %&gt;%\n  row_spec(5, bold = T, background = \"lightgreen\")\n\n\n\n\n\nThreshold\nSensitivity (%)\nSpecificity (%)\nAccuracy (%)\nPredicted Failures\n\n\n\n\n0.10\n22.99\n88.89\n23.45\n2571\n\n\n0.15\n25.38\n89.53\n36.04\n2159\n\n\n0.20\n28.72\n85.91\n51.45\n1560\n\n\n0.25\n31.31\n83.68\n60.68\n1137\n\n\n0.30\n33.83\n81.48\n67.83\n742\n\n\n0.35\n36.56\n79.10\n73.66\n331\n\n\n0.40\n37.07\n77.76\n75.94\n116\n\n\n0.45\n31.91\n77.26\n76.44\n47\n\n\n0.50\n26.32\n77.12\n76.75\n19\n\n\n0.55\n28.57\n77.11\n76.98\n7\n\n\n0.60\n33.33\n77.11\n77.06\n3\n\n\n\n\n\n\n\nHere, we examine the confusion matrix for the test dataset predictions for our selected model (i.e: Model 3) at the selected thresholds (0.3). The confusion matrix compares our predictions to the actual outcomes. In the confusion matrix, the value in the top left corner represents cases where we correctly predicted a restaurant would pass inspection. The value in the bottom right corner indicates the number of restaurants we correctly predicted as passing inspection. The value in the top right corner, indicates the number of restaurants we predicted as Failing but actually passed inspection (i.e: false negatives). The value in the bottom left corner indicates the number of restaurants we predicted to fail that actually passed (i.e: false positives). When using the 0.3 threshold, our model miss classifies 491 restaurants as failing inspections and correctly classifies 255 restaurants resulting in a sensitivity of 34.18%.\n\n\nCode\npredictions &lt;- as.factor(ifelse(testProbs$probs3 &gt; 0.3,1,0))\nconfusematrix &lt;- caret::confusionMatrix(testProbs$Outcome,predictions,positive=\"1\")\n\nconfusematrix$table %&gt;%\n  data.frame() %&gt;%\n  spread(key=Reference,value=Freq) %&gt;%\n  rename('Pass' = '0', 'Fail' = '1') %&gt;%\n  mutate(Total = Pass + Fail,\n         Prediction = c('Pass','Fail')) %&gt;%\n  kbl() %&gt;%\n  add_header_above(header=c(\" \" = 1,\"Actual\" = 2,\" \" = 1)) %&gt;%\n  kable_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nActual\n\n\n\n\nPrediction\nPass\nFail\nTotal\n\n\n\n\nPass\n1505\n491\n1996\n\n\nFail\n342\n251\n593"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#cross-validation",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#cross-validation",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Cross Validation",
    "text": "Cross Validation\nWe also run a cross validation for our chosen model and threshold combination. The cross validation helps us understand if the model is generalizable to different data. A cross validation involves running the model multiple times while changing the data points which are included in the training and test datasets. We run a 50-fold cross validation using the 2021 inspection data and compare the predicted outcomes to the actual outcome in each of the 50 test datasets and calculate the sensitivity, specificty, accuracy, and area under the curve metrics for each fold.\nThe figure below shows the distribution of the AUC, sensitivity, specificity, and accuracy across the 50 folds. Sensitivity, specificity, and accuracy are again calculated using the 0.3 probability threshold. The model generalizes well according to the specificity and accuracy outcomes as indicated by the limited variability in the range of outcomes. However, the sensitivity (i.e: percent of failures correctly predicted) has a wider spread of outcomes across the different folds. Understanding this variation in the models ability to accurately predict restaurant failures is a key limitation which will be important for the restaurant improvement program to understand. In the next section, we will try to identify geographic areas where the model performance is poor.\n\n\nCode\nctrl &lt;- trainControl(method = \"cv\", number = 50, classProbs = TRUE, savePredictions = TRUE, summaryFunction = twoClassSummary)\n\ncvFit &lt;- train(\n  results ~ age + prev_fails + Median_Rent + rodents.nn + sanitation.nn + thefts.nn + risk + pct_poverty + prev_year_pct_fail + decile + risk,\n  data = data_2021_2,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = \"ROC\",\n  trControl = ctrl\n)\n\n#This is doing the cross validation with a custom threshold, using a treshold of 0.5 is not useful and could not figure out how to use a different threshold with the summaryFunction\n\npred &lt;- cvFit$pred %&gt;%\n  mutate(pred = (ifelse(Fail &gt; 0.30,'Fail','Pass')),\n        type = case_when(pred == 'Fail' & obs == 'Fail' ~ 'TP',\n                         pred == 'Pass' & obs == 'Pass' ~ 'TN',\n                         pred == 'Fail' & obs == 'Pass' ~ 'FP',\n                         pred == 'Pass' & obs == 'Fail' ~ 'FN')) %&gt;%\n  group_by(Resample,type) %&gt;% tally %&gt;%\n  spread(key=type,value=n) %&gt;%\n  mutate(Sensitivity = TP / (TP + FN),\n         Specificity = TN / (FP + TN),\n         Accuracy = (TP + TN) / (TP + TN + FP + FN)) %&gt;%\n  inner_join(.,cvFit$resample %&gt;% dplyr::select(Resample,ROC),by='Resample') %&gt;%\n  dplyr::select('Sensitivity','Specificity','Accuracy','ROC') %&gt;%\n  rename(Area_Under_Curve = 'ROC') %&gt;%\n  pivot_longer(cols=c('Sensitivity','Specificity','Accuracy','Area_Under_Curve'),names_to='metric',values_to='value') %&gt;%\n  group_by(metric) %&gt;%\n  mutate(mean = mean(value))\n\nggplot(data=pred,aes(value)) + \n  geom_histogram(bins=50, fill = '#E4002B')+\n  facet_wrap(~metric) +\n  geom_vline(aes(xintercept = mean), colour = \"#41B6E6\", linetype = 3, size = 1.5) +\n  scale_x_continuous(limits = c(0, 1)) +\n  labs(x=\"Goodness of Fit\", y=\"Count\", title=\"Histogram of Cross Validation Model Fit Metrics\",subtitle = \"Across-fold mean represented as dotted lines\")+\n  theme_bw()"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#accuracy-by-neighboorhood",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#accuracy-by-neighboorhood",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Accuracy by Neighboorhood",
    "text": "Accuracy by Neighboorhood\nThe maps below show the sensitivity, specificity, and accuracy of the model predictions for the initial test dataset by neighborhood when using the 0.3 threshold. There are some key spatial patterns and trends in the results which are important for model users to understand. First, the sensitivity in many neighborhoods is 0%. This means that in many parts of the city the model failed to accurately predict all restaurant failures when setting the threshold for a predicted failure at 0.3. Examples of neighborhoods where this is true include Grand Boulevard, United Center, Armour Square and Logan Square. The neighborhoods which have a sensitivity of 0% tend to also have a specificity near 100%.\nConversely, there are also neighborhoods where the specificity is 0% and the sensitivity is near 100%. Examples of such neighborhoods include Chinatown, New City, Garfield Park and Washington Heights. These are the same neighborhoods which had high failure rates based on our exploratory analysis.\n\n\nCode\naccuracy_neigh &lt;- testProbs %&gt;%\n  select(inspection_id,Outcome,probs3) %&gt;%\n  left_join(.,test %&gt;% dplyr::select(pri_neigh,inspection_id),by='inspection_id') %&gt;%\n  mutate(predoutcome = ifelse(probs3 &gt; 0.30,1,0),\n         type = case_when(Outcome == 0 & predoutcome == 0 ~ 'TN',\n                          Outcome == 1 & predoutcome == 1 ~ 'TP',\n                          Outcome == 1 & predoutcome == 0 ~ 'FN',\n                          Outcome == 0 & predoutcome == 1 ~ 'FP')) %&gt;%\n  group_by(pri_neigh,type) %&gt;% tally() %&gt;%\n  spread(key=type,value=n) %&gt;%\n  replace(is.na(.), 0) %&gt;%\n  mutate(Sensitivity = TP / (TP + FN),\n         Specificity = TN / (FP + TN),\n         Accuracy = (TP + TN) / (TP + TN + FP + FN)) %&gt;%\n  select(Sensitivity,Specificity,Accuracy,pri_neigh) %&gt;%\n  gather(accuracy_measure,value,-pri_neigh) %&gt;%\n  inner_join(neighboorhoods,.,by='pri_neigh')\n  \n  ggplot(data=accuracy_neigh)+\n  geom_sf(aes(fill=value * 100))+\n  facet_wrap(~accuracy_measure)+\n  scale_fill_distiller(palette = 'Blues',name='Percentage',trans='reverse')+\n  theme_bw()+\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(),\n        panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank())"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#accuracy-by-neighboorhood-type",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#accuracy-by-neighboorhood-type",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Accuracy by Neighboorhood Type",
    "text": "Accuracy by Neighboorhood Type\nTo better understand how sensitivity, specificity, and accuracy vary across different types of neighborhoods we group all neighborhoods in Chicago into four categories based on the restaurant inspection failure rate for restaurants in 2021. Each category contains the same number of neighborhoods.\nOur predictions for restaurants in neighborhoods that have a very low or low historical failure rate exhibit a low sensitivity, high specificity, and high overall accuracy. The low sensitivity indicates that our model only accurately predicts 1.6% of restaurant failures in neighborhoods with a low historical failures. Because the number of failures is very small (approximately 8-12% of inspections in most neighborhoods) our accuracy is still high in these neighborhoods.\nOur predictions for restaurants in neighborhoods that have a high historical failure rate exhibit a high sensitivity and low specificity, and low overall accuracy. The low specificity indicates that our model only accurately predicts 21.71% of restaurants as passing inspections. A large number of restaurants that actually passed inspections are miss classified as failing inspections in neighborhoods with a high historical failure rate.\n\n\nCode\ntestProbs %&gt;%\n  select(inspection_id,Outcome,probs3) %&gt;%\n  left_join(.,test %&gt;% dplyr::select(quartile,inspection_id),by='inspection_id') %&gt;%\n  mutate(predoutcome = ifelse(probs3 &gt; 0.30,1,0),\n         type = case_when(Outcome == 0 & predoutcome == 0 ~ 'TN',\n                          Outcome == 1 & predoutcome == 1 ~ 'TP',\n                          Outcome == 1 & predoutcome == 0 ~ 'FN',\n                          Outcome == 0 & predoutcome == 1 ~ 'FP')) %&gt;%\n  group_by(quartile,type) %&gt;% tally() %&gt;%\n  ungroup() %&gt;%\n  spread(key=type,value=n) %&gt;%\n  replace(is.na(.), 0) %&gt;%\n  mutate(Sensitvity = round((TP / (TP + FN)) * 100,2),\n         Specificity = round((TN / (FP + TN)) * 100,2),\n         Accuracy = round(((TP + TN) / (TP + TN + FP + FN)) * 100,2),\n         Neighboor = c('Very Low (Quartile 1)','Low (Quartile 2)','Medium (Quartile 3)','High (Quartile 4)')) %&gt;%\n  select(Neighboor,Sensitvity,Specificity,Accuracy) %&gt;%\n  kbl(col.names = c('Historical Neighboorhood Failure Rate','Sensitivity (%)','Specificity (%)','Accuracy (%)')) %&gt;%\n  kable_minimal()\n\n\n\n\n\nHistorical Neighboorhood Failure Rate\nSensitivity (%)\nSpecificity (%)\nAccuracy (%)\n\n\n\n\nVery Low (Quartile 1)\n1.61\n99.80\n88.99\n\n\nLow (Quartile 2)\n1.60\n97.96\n79.85\n\n\nMedium (Quartile 3)\n25.33\n82.69\n68.08\n\n\nHigh (Quartile 4)\n82.03\n21.90\n41.84"
  },
  {
    "objectID": "posts/Chicago_Inspections/Chicago_Restaurants.html#accuracy-by-race-context",
    "href": "posts/Chicago_Inspections/Chicago_Restaurants.html#accuracy-by-race-context",
    "title": "Predicting Restaurant Inspection Failures in Chicago",
    "section": "Accuracy by Race Context",
    "text": "Accuracy by Race Context\nWe also examine our model performance across different race contexts. We classify census tracts into majority white census tracts and majority non white census tracts and examine the accuracy, sensitivity, and specificity for predictions for restaurants in majority white and majority non white census tracts using our selected threshold of 0.3. The sensitivity is higher for restaurants in majority non-white areas and the specificity is lower. The lower specificity our model and threshold combination tends to overestimate the number of restaurant failures in non-white areas. Conversely, the low sensitivity in white areas indicates that the model misses a large percentage of restaurant failures in majority white area.\n\n\nCode\ntestProbs %&gt;%\n  select(inspection_id,Outcome,probs3) %&gt;%\n  left_join(.,test %&gt;% dplyr::select(pct_non_white,inspection_id),by='inspection_id') %&gt;%\n  mutate(predoutcome = ifelse(probs3 &gt; 0.30,1,0),\n         majority_pop = ifelse(pct_non_white &gt; 50,'Non-White','White'),\n         type = case_when(Outcome == 0 & predoutcome == 0 ~ 'TN',\n                          Outcome == 1 & predoutcome == 1 ~ 'TP',\n                          Outcome == 1 & predoutcome == 0 ~ 'FN',\n                          Outcome == 0 & predoutcome == 1 ~ 'FP')) %&gt;%\n  group_by(majority_pop,type) %&gt;% tally() %&gt;%\n  ungroup() %&gt;%\n  spread(key=type,value=n) %&gt;%\n  replace(is.na(.), 0) %&gt;%\n  mutate(Sensitvity = round((TP / (TP + FN)) * 100,2),\n         Specificity = round((TN / (FP + TN)) * 100,2),\n         Accuracy = round(((TP + TN) / (TP + TN + FP + FN)) * 100,2)) %&gt;%\n  select(majority_pop,Sensitvity,Specificity,Accuracy) %&gt;%\n  kbl(col.names = c('Majority Population','Sensitivity (%)','Specificity (%)','Accuracy (%)')) %&gt;%\n  kable_minimal()\n\n\n\n\n\nMajority Population\nSensitivity (%)\nSpecificity (%)\nAccuracy (%)\n\n\n\n\nNon-White\n55.56\n62.91\n60.89\n\n\nWhite\n32.55\n81.65\n71.63"
  },
  {
    "objectID": "posts/PA_wind_mapping/PA_Wind.html",
    "href": "posts/PA_wind_mapping/PA_Wind.html",
    "title": "Developing a Wind Suitability Map for Pennsylvania",
    "section": "",
    "text": "Overview\n\n\n\n\nThis project was completed as a final project for ENMG 5020 - Introduction to Energy Policy. The project is a policy memo directed to the office of the Governor of Pennsylvania encouraging his office to expand the amount of wind energy in Pennsylvania. A key pieces of memo is highlighting the results of a wind suitability mapping analysis I completed for the state of Pennsylvania. The analysis uses multi-criteria decision making (MCDM) techniques which was carried out using ArcGIS Pro. I identify areas suitable for wind turbine development based on five main criteria: Wind Speed, Land Cover, Population Density, Distance to Roads, and Distance to existing transmission lines. Transmission\n\n\n\n\n\n\nPrior to carrying out my analysis I conducted an literature review of previous wind suitability mapping techniques, and based my own methods on the outcomes of the literature review. The map below shows the results of the wind suitability mapping work. The map includes an overlay showing important bird areas and state lands. Important bird areas are included due to concerns about building wind turbines on important bird areas. State lands are included in the map in order to highlight the large amount of state land that is suitable for wind turbine development.\nMy full policy memo is available for review and download below."
  },
  {
    "objectID": "posts/philly_flood_census_mapping/index.html",
    "href": "posts/philly_flood_census_mapping/index.html",
    "title": "Mapping Population Vulnerability to Flooding in Philadelphia",
    "section": "",
    "text": "This analysis examines the estimated population living in census block groups in Philadelphia that overlap flood prone areas. The analysis only examines costal and riverine flooding and does not capture infrastructure flooding or sea level rise from climate change. Flood prone areas are defined as areas within FEMA’s 500-year flood plain. The population data used comes from the decennial and 5 year American community survey (ACS) datasets which are maintained by the U.S Census Bureau. This analysis breaks the population down by race and by geographic section of the city. The boundaries used to define geographic areas of the city come from the Philadelphia Open Data Portal.\nThis analysis is also availble as a PDF report."
  },
  {
    "objectID": "posts/philly_flood_census_mapping/index.html#get-data",
    "href": "posts/philly_flood_census_mapping/index.html#get-data",
    "title": "Mapping Population Vulnerability to Flooding in Philadelphia",
    "section": "Get Data",
    "text": "Get Data\nThe first step in my analysis is to get census data. I choose to use five-year ACS data from 2021, 2018 and 2015 and decennial census from 2000 and 2010. The data I import includes the total population, and the population breakdown by race for all census tracts in Philadelphia. The census data is downloaded using cenpy. Because this analysis uses decennial and acs datasets it was necessary to determine the appropriate variables in both the ACS and the deccenial data prior to carrying out my analysis. I did this using a combination of cenpy functions and Social Explorer.\n\n\nCode\n#Make API Connections to different ACS datasets\nacs2021 = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\nacs2018 = cenpy.remote.APIConnection(\"ACSDT5Y2018\")\nacs2015 = cenpy.remote.APIConnection(\"ACSDT5Y2015\")\ndec2010 = cenpy.remote.APIConnection(\"DECENNIALSF12010\")\ndec2000 = cenpy.remote.APIConnection(\"DECENNIALSF12000\")\n\n#Define ACS Variables in each dataset\nvariables_acs = ['NAME',\n               'B03002_001E',#Total Pop\n              'B03002_003E', #White Pop\n              'B03002_004E', #Black Pop\n              'B03002_005E', #American Indian Pop\n              'B03002_006E', #Asian Pop\n              'B03002_007E', #Hawian Native Pop\n              'B03002_008E', #Other Race\n              'B03002_009E',#Some other race\n                'B03002_012E' ] #Hispanic\n\nvariables_dec2010 = ['NAME',\n                    'P005001', #Total Pop\n                    'P005003', #White Pop\n                    'P005004', #Black Pop\n                    'P005005',#American Indian Pop\n                    'P005006', #Asian Pop\n                    'P005007', #Native Hawian,\n                    'P005008', #Some other race\n                    'P005009',#Two or more races\n                    'P005010']#Hispanic\n\nvariables_dec2000 = ['NAME',\n                     'P004001', #Total Pop\n                     'P004002', #Hispanic or Latino\n                     'P004005', #White\n                     'P004006', #Black\n                     'P004007', #American Indian Pop\n                     'P004008', #Asian\n                     'P004009', #Native Hawian\n                     'P004010', #Some other race\n                     'P004011'] #Two or more race\n\n#Download ACS Data at block group level for each year for variables of interest\n\ndf_acs2021 = acs2021.query(\n    cols=variables_acs,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": \"42\", \"county\": \"101\", \"tract\": \"*\"},\n)\n\ndf_acs2018 = acs2018.query(\n    cols=variables_acs,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": \"42\", \"county\": \"101\", \"tract\": \"*\"},\n)\n\ndf_acs2015 = acs2015.query(\n    cols=variables_acs,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": \"42\", \"county\": \"101\", \"tract\": \"*\"},\n)\n\ndf_census2010 = dec2010.query(\n    cols=variables_dec2010,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": \"42\", \"county\": \"101\", \"tract\": \"*\"},\n)\n\ndf_census2000= dec2000.query(\n    cols=variables_dec2000,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": \"42\", \"county\": \"101\", \"tract\": \"*\"},\n)"
  },
  {
    "objectID": "posts/philly_flood_census_mapping/index.html#clean-census-data",
    "href": "posts/philly_flood_census_mapping/index.html#clean-census-data",
    "title": "Mapping Population Vulnerability to Flooding in Philadelphia",
    "section": "Clean Census Data",
    "text": "Clean Census Data\nMy next step is to clean the census data. Because cenpy returns population data as text it is necessary to convert all the numeric columns to floats. After converting the data to floats, I rename my columns of interest so that they are easier for me to remember. Additionally, I group together the population which is not White, Asian, Black, or Hispanic into an ‘Other_Pop’ group. The other pop group includes Native Americans, Native Hawaiians, other races, and residents who identified as multi-racial.\nIn order to streamline my data cleaning procedure I created functions which include the data cleaning steps. The clean_acs function is used to clean the acs data, while the clean_dec2010 function is used to clean the 2010 decennial data, and the clean_dec2000 function is used to clean the 2000 decennial data.\n\n\nCode\ndef clean_acs(df,year):\n    for variable in variables_acs:\n        if variable != \"NAME\":\n            df[variable] = df[variable].astype(float)\n    df.rename({'B03002_001E':'Total_Pop','B03002_003E':'White_Pop','B03002_004E':'Black_Pop','B03002_012E':'Hispanic_Pop','B03002_006E':'Asian_Pop'},inplace=True,axis=1)\n    df['year'] = year\n    other_pop_list = ['B03002_005E','B03002_007E','B03002_008E','B03002_009E']\n    df['Other_Pop'] = df[other_pop_list].sum(axis=1)\n    df.drop(other_pop_list,inplace=True,axis=1)\n    return df\n\ndef clean_dec2010(df,year):\n    for variable in variables_dec2010:\n        if variable != \"NAME\":\n            df[variable] = df[variable].astype(float)\n    df.rename({'P005001':'Total_Pop','P005003':'White_Pop','P005004':'Black_Pop','P005010':'Hispanic_Pop','P005006':'Asian_Pop'},inplace=True,axis=1)\n    df['year'] = year\n    other_pop_list = ['P005005','P005007','P005008','P005009']\n    df['Other_Pop'] = df[other_pop_list].sum(axis=1)\n    df.drop(other_pop_list,inplace=True,axis=1)\n    return df\n\ndef clean_dec2000(df,year):\n    for variable in variables_dec2000:\n        if variable != \"NAME\":\n            df[variable] = df[variable].astype(float)\n    df.rename({'P004001':'Total_Pop','P004005':'White_Pop','P004006':'Black_Pop','P004002':'Hispanic_Pop','P004008':'Asian_Pop'},inplace=True,axis=1)\n    df['year'] = year\n    other_pop_list = ['P004007','P004009','P004010','P004011']\n    df['Other_Pop'] = df[other_pop_list].sum(axis=1)\n    df.drop(other_pop_list,inplace=True,axis=1)\n    return df\n\ndf_acs2021 = clean_acs(df_acs2021,2021)\ndf_acs2018 = clean_acs(df_acs2018,2018)\ndf_acs2015 = clean_acs(df_acs2015,2015)\n\ndf_census2010 = clean_dec2010(df_census2010,2010)\ndf_census2000 = clean_dec2000(df_census2000,2000)"
  },
  {
    "objectID": "posts/philly_flood_census_mapping/index.html#get-block-group-boundaries-and-join-to-census-data",
    "href": "posts/philly_flood_census_mapping/index.html#get-block-group-boundaries-and-join-to-census-data",
    "title": "Mapping Population Vulnerability to Flooding in Philadelphia",
    "section": "Get Block Group Boundaries and Join to Census Data",
    "text": "Get Block Group Boundaries and Join to Census Data\nThe next step is to download the block groups which are needed to map my data. The ACS data from 2021 uses the 2020 block groups, while the ACS data for 2018, 2015 and the 2010 decennial census data utilize the 2010 census block groups. The 2000 decennial census data utilizes the 2000 census blocks.\nThe 2020 census block group boundaries are obtained using pygris. Unfortunately, I was not able to download the 2010 and 2020 census block group boundaries using pygris. Thus, I obtained the 2010 block group boundaries from open data philly. The 2000 census boundaries were downloaded from the census.gov and I saved the boundaries as a shapefile on my computer and read the shapefile into python using geopandas.\n\n\nCode\nphilly_code='101'\nstate_code='42'\n\nphilly_2020_block_groups = pygris.block_groups(\n    state=state_code, county=philly_code, year=2020\n).to_crs(6565)\n\n#Read in 2010 block groups from Open Data Philly because I could not get pygris to work with 2010 as year\nphilly_2010_block_groups = gpd.read_file('https://opendata.arcgis.com/datasets/2f982bada233478ea0100528227febce_0.geojson').to_crs(6565)\n\nphilly_2010_block_groups.rename({'GEOID10':'GEOID'},inplace=True,axis=1)\n\n#Downloaded 2000 block groups from census.gov - could not get pygris to work and 2000 not available on Open Data Philly.\n\nphilly_2000_block_groups = gpd.read_file('./data/Philly_2000_census.shp').drop('NAME',axis=1).to_crs(6565)\n\n\nAfter obtaining all the necessary census boundaries I join each of the census datasets to the appropriate geospatial boundaries using the pandas merge function.\n\n\nCode\ndf_acs2021_g = philly_2020_block_groups.merge(\n    df_acs2021,\n    left_on=[\"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\"],\n    right_on=[\"state\", \"county\", \"tract\", \"block group\"],\n)\n\ndf_acs2018_g = philly_2010_block_groups.merge(\n    df_acs2018,\n    left_on=[\"STATEFP10\", \"COUNTYFP10\", \"TRACTCE10\", \"BLKGRPCE10\"],\n    right_on=[\"state\", \"county\", \"tract\", \"block group\"],\n)\n\ndf_acs2015_g = philly_2010_block_groups.merge(\n    df_acs2015,\n    left_on=[\"STATEFP10\", \"COUNTYFP10\", \"TRACTCE10\", \"BLKGRPCE10\"],\n    right_on=[\"state\", \"county\", \"tract\", \"block group\"],\n)\n\ndf_census2010_g = philly_2010_block_groups.merge(\n    df_census2010,\n    left_on=[\"STATEFP10\", \"COUNTYFP10\", \"TRACTCE10\", \"BLKGRPCE10\"],\n    right_on=[\"state\", \"county\", \"tract\", \"block group\"],\n)\n\ndf_census2000_g = philly_2000_block_groups.merge(\n    df_census2000,\n    left_on=[\"STATE\", \"COUNTY\", \"TRACT\", \"BLKGROUP\"],\n    right_on=[\"state\", \"county\", \"tract\", \"block group\"],\n)"
  },
  {
    "objectID": "posts/philly_flood_census_mapping/index.html#merge-data-for-all-years",
    "href": "posts/philly_flood_census_mapping/index.html#merge-data-for-all-years",
    "title": "Mapping Population Vulnerability to Flooding in Philadelphia",
    "section": "Merge data for all years",
    "text": "Merge data for all years\nNext, I trim the collums included in each of the census datasets so that they all include the same columns. This is necessary in order to merge all the datasets into one geodataframe. After trimming each dataset I use the pandas concatenate function to merge together the data for all five years. The merged geodataframe also includes the geometry column to facilitate geospatial analysis and mapping.\n\n\nCode\n\nkeep_cols = ['NAME','state','county','block group','tract','year','Total_Pop','White_Pop','Black_Pop','Asian_Pop','Hispanic_Pop','Other_Pop','geometry']\n\ndf_acs2021_g = df_acs2021_g[keep_cols]\ndf_acs2018_g = df_acs2018_g[keep_cols]\ndf_acs2015_g = df_acs2015_g[keep_cols]\ndf_census2010_g = df_census2010_g[keep_cols]\ndf_census2000_g = df_census2000_g[keep_cols]\n\nall_years = pd.concat([df_acs2021_g,df_acs2018_g,df_acs2015_g,df_census2010_g,df_census2000_g]).reset_index(drop=True)"
  },
  {
    "objectID": "posts/philly_flood_census_mapping/index.html#covert-to-centroid",
    "href": "posts/philly_flood_census_mapping/index.html#covert-to-centroid",
    "title": "Mapping Population Vulnerability to Flooding in Philadelphia",
    "section": "Covert to centroid",
    "text": "Covert to centroid\nNext, I convert the merged census block group dataset from polygons to point. The centroid of each census block group is used as the point geometry. This step is necessary because I use the centroid to determine if a census block group is adjacent to the floodplain.\n\n\nCode\n# copy GeoDataFrame\nall_years['infloodplain'] = False\nall_years_points = all_years.copy()\n# change geometry \nall_years_points['geometry'] = all_years_points['geometry'].centroid"
  },
  {
    "objectID": "posts/philly_flood_census_mapping/index.html#identify-census-blocks-adjacent-to-the-floodplain",
    "href": "posts/philly_flood_census_mapping/index.html#identify-census-blocks-adjacent-to-the-floodplain",
    "title": "Mapping Population Vulnerability to Flooding in Philadelphia",
    "section": "Identify Census Blocks Adjacent to the Floodplain",
    "text": "Identify Census Blocks Adjacent to the Floodplain\nThe next step is to identify which census block groups that are adjacent to the floodplain. I use a spatial join to join the census block group centroids to the buffered floodplain and identify which census block groups centroids are located within the buffered floodplain. I add a new column called “infloodplain” to the merged census block group geodataframe and set this equal to True for census tracts that have a centroid overlapping the buffered floodplain.\nOne challenege with working with different census block group layers is the census block group boundaries have changed over time. In order to account for this, I manually assign multiple census block groups as located near the floodplain to ensure that our map of areas located near the floodplain is roughly the same across all five years of analysis. It is not possible to get perfect allignment across all census block group datasets due to some larger shifts in the census block group boundaries. This is espically true in the shift from the 2000 census block group boundaries to the 2010 census block group boundaries.\n\n\nCode\n#Determine which census tract centroids overlap the flood plain buffer\ninfloodplain = all_years_points.sjoin(buffer2,how='inner').index\n\nall_years.iloc[[infloodplain],[13]] = True\n\n\"\"\"\nThere were a much larger number of census blocks in 2000 - \nin order to insure comparability between years I had to reassign several census blocks to be in the flood plain manually \n(or not in the flood plain)\n\"\"\"\n\ndef set_inflood_true(df,year,tract,block):\n    filt = (df['year'] == year) & (df['tract'] == tract) & (df['block group'] == block)\n    df.loc[filt,'infloodplain'] = True\n    \ndef set_inflood_false(df,year,tract,block):\n    filt = (df['year'] == year) & (df['tract'] == tract) & (df['block group'] == block)\n    df.loc[filt,'infloodplain'] = False\n    \nset_inflood_true(all_years,2000,'0328','1')\nset_inflood_true(all_years,2000,'0351','1')\nset_inflood_true(all_years,2000,'0068','9')\nset_inflood_true(all_years,2000,'0035','1')\nset_inflood_true(all_years,2000,'0034','1')\nset_inflood_true(all_years,2000,'0124','1')\nset_inflood_true(all_years,2000,'0150','9')\nset_inflood_true(all_years,2000,'0034','2')\nset_inflood_true(all_years,2000,'0089','2')\nset_inflood_true(all_years,2000,'0089','3')\nset_inflood_true(all_years,2000,'0223','2')\nset_inflood_true(all_years,2000,'0209','3')\nset_inflood_true(all_years,2000,'0033','1')\nset_inflood_true(all_years,2000,'0013','5')\nset_inflood_true(all_years,2000,'0143','3')\nset_inflood_true(all_years,2000,'0142','3')\nset_inflood_true(all_years,2000,'0142','5')\nset_inflood_true(all_years,2000,'0065','7')\nset_inflood_true(all_years,2000,'0065','4')\nset_inflood_true(all_years,2000,'0099','1')\nset_inflood_true(all_years,2000,'0116','3')\nset_inflood_true(all_years,2000,'004102','2')\nset_inflood_true(all_years,2000,'0207','5')\nset_inflood_true(all_years,2000,'0183','3')\nset_inflood_true(all_years,2000,'0184','2')\nset_inflood_true(all_years,2000,'0142','2')\nset_inflood_true(all_years,2000,'0159','2')\nset_inflood_true(all_years,2000,'0160','7')\nset_inflood_true(all_years,2000,'0186','7')\nset_inflood_true(all_years,2000,'0295','1')\nset_inflood_true(all_years,2000,'0294','4')\nset_inflood_true(all_years,2000,'0183','4')\nset_inflood_true(all_years,2000,'0293','2')\n\nset_inflood_true(all_years,2010,'032900','1')\nset_inflood_true(all_years,2015,'032900','1')\nset_inflood_true(all_years,2018,'032900','1')\nset_inflood_true(all_years,2010,'033690','1')\nset_inflood_true(all_years,2015,'033690','1')\nset_inflood_true(all_years,2018,'033690','1')\nset_inflood_true(all_years,2010,'036900','1')\nset_inflood_true(all_years,2015,'036900','1')\nset_inflood_true(all_years,2018,'036900','1')\nset_inflood_true(all_years,2010,'036900','2')\nset_inflood_true(all_years,2015,'036900','2')\nset_inflood_true(all_years,2018,'036900','2')\nset_inflood_true(all_years,2010,'980000','1')\nset_inflood_true(all_years,2015,'980000','1')\nset_inflood_true(all_years,2018,'980000','1')\n\nset_inflood_true(all_years,2021,'989100','2')\nset_inflood_true(all_years,2021,'980701','2')\nset_inflood_true(all_years,2021,'980901','4')\nset_inflood_true(all_years,2021,'036902','4')\nset_inflood_true(all_years,2021,'037500','1')\nset_inflood_true(all_years,2021,'014201','2')\nset_inflood_true(all_years,2021,'980903','1')\nset_inflood_true(all_years,2021,'003300','2')\nset_inflood_true(all_years,2021,'003300','2')\nset_inflood_true(all_years,2021,'035100','2')\n\nset_inflood_false(all_years,2021,'035900','2')\nset_inflood_false(all_years,2021,'036000','3')\nset_inflood_false(all_years,2021,'980300','1')\n\n\nset_inflood_false(all_years,2000,'0361','1')\nset_inflood_false(all_years,2000,'0055','1')\nset_inflood_false(all_years,2000,'0044','1')\n\nset_inflood_false(all_years,2010,'035900','2')\nset_inflood_false(all_years,2015,'035900','2')\nset_inflood_false(all_years,2018,'035900','2')"
  },
  {
    "objectID": "posts/philly_flood_census_mapping/index.html#geographic-breakdown",
    "href": "posts/philly_flood_census_mapping/index.html#geographic-breakdown",
    "title": "Mapping Population Vulnerability to Flooding in Philadelphia",
    "section": "Geographic Breakdown",
    "text": "Geographic Breakdown\nNext, we analyze the patterns by region to see if there are differences in the trends across the different geographic areas of Philadelphia. Data on planning districts is downloaded from open data Philadelphia, and the planning districts are grouped into seven larger geographic areas: River Wards, Northeast, Northwest, Center City, West & North, South, and Southwest.\nThe census block groups are then assigned to the geographic area they are situated in and a new column which contains the name of the geographic area is added to the flood tracts dataframe.\n\n\nCode\narea = {'River Wards':'River Wards',\n'North Delaware':'Northeast',\n'Lower Far Northeast':'Northeast',\n'Central':'Center City',\n'University Southwest':'West & North',\n'Upper Northwest':'Northwest',\n'Upper North':'Northwest',\n'South':'South',\n'North':'West & North',\n'Lower Northwest':'Northwest',\n'Lower South':'South',\n'Lower Northeast':'Northeast',\n'Central Northeast':'Northeast',\n'West':'West & North',\n'Upper Far Northeast':'Northeast',\n'Lower Southwest':'Southwest',\n'West Park':'West & North',\n'Lower North':'West & North'\n}\n\nplanning_districts = gpd.read_file('https://opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson').to_crs(6565)\n\nplanning_districts['region'] = planning_districts['DIST_NAME'].map(area)\n\npoints_flood = gpd.GeoDataFrame(geometry=flood_tracts['geometry'].centroid)\n\npoints_flood = points_flood.sjoin(planning_districts,how='inner')\npoints_flood = points_flood[['region']]\n\nflood_tracts_area = flood_tracts.merge(points_flood,left_index=True, right_index=True)\n\n\n\nThe map on the left below shows how the planning districts have been divided into geographic areas which are used for the analysis. The map on the right shows the census block groups that are located adjacent to the flood plain colored according to the geographic area they are located in.\n\n\nCode\n%matplotlib inline\n\nfig = plt.figure(figsize=(8, 3.5), layout=\"constrained\")\nspec = fig.add_gridspec(ncols=2, nrows=1)\n\nax0 = fig.add_subplot(spec[0, 0])\n\nplanning_districts.plot(ax=ax0,column=\"region\")\n\nax1 = fig.add_subplot(spec[0, 1])\n\nflood_tracts_area.plot(ax=ax1,column=\"region\",legend=True,legend_kwds={\"loc\": \"center left\", \"bbox_to_anchor\": (1, 0.5)})\n\nax1.set_axis_off()\nax1.set_aspect(\"equal\")\n\nax0.set_axis_off()\nax0.set_aspect(\"equal\")\n\n\n\n\n\nThe chart below shows the trend in the population living in census block groups located near the floodplain by region of the city. Some notable patterns include a large expansion in the population in center city living in census block groups located near the flood hazard zone. Between 2015 and 2021 there was also increase in the population living in census block groups located near the flood hazard zone in Northeast, Northwest, and South Philadelphia.\n\n\nCode\nflood_tracts_melt = pd.melt(flood_tracts_area,id_vars=['block group','year','region'],\n                            value_vars=['White_Pop','Black_Pop','Asian_Pop','Hispanic_Pop','Other_Pop'],\n                           var_name='Race',value_name='Population')\n                                                       \nalt.Chart(flood_tracts_melt).mark_line(point=True).encode(\n    alt.X('year:N').title('Year'),\n    alt.Y('sum(Population)').title('Total Population'),\n    color='region',\n    tooltip=[\"year\", \"region\", \"sum(Population)\"]\n).properties(\n    width=500,\n    height=300,\n    title='Population Living in Census Block Groups Near Floodplain by Area'\n)"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html",
    "href": "posts/TOD_Washington_DC/DCTOD.html",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "",
    "text": "Transited oriented development (TOD) refers to the trend of creating vibrant, livable communities which are compact, walkable, and centered around transit systems. Washington DC has experienced TOD in many sections of the city with new apartment buildings being constructed near metro stops throughout the city. This memo attempts to look at the impacts of TOD development on income and rent. We will also examine if residents living in the TOD are more likely to commute to work on public transit. We look at the spatial patterns of where workers working in public administration tend to live, and examine if they are more likely to live in neighborhoods near the metro. Data on income, rent, and mode of transportation used for commuting is downloaded from the American Community Survey and we compare data from 2009 to data from 2017.\nThe analysis starts by identifying the TOD area. This analysis defines the TOD area as including all census tracts whose center is located within 1/2 a mile of a metro stop.\nThis analysis focuses exclusively on metro stops and does not consider bus stops. This is because metro is generally considered faster, more reliable form of public transportation and residents are more likely to be willing to pay more in rent to live near a metro stop. Additionally, transit oriented development in Washington DC has tended occur around metro stops not bus stops."
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#buffer-metro-points",
    "href": "posts/TOD_Washington_DC/DCTOD.html#buffer-metro-points",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Buffer Metro Points",
    "text": "Buffer Metro Points\nI create a one mile buffer around the dc metro points.\n\n\nCode\nmetro_buffer &lt;- st_buffer(dcmetro,2640)\nmetro_union_buffer &lt;- st_union(metro_buffer)"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#get-the-centroids-of-the-census-tracts-and-identify-tod-and-non-tod",
    "href": "posts/TOD_Washington_DC/DCTOD.html#get-the-centroids-of-the-census-tracts-and-identify-tod-and-non-tod",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Get the centroids of the census tracts and identify TOD and Non-TOD",
    "text": "Get the centroids of the census tracts and identify TOD and Non-TOD\nI get the centroid of the census tracts and determine which census tract centroids are located within the TOD area. Census tracts which have a centroid within the TOD area are considered to be part of the TOD area.\n\n\nCode\ncentroids &lt;- st_centroid(all_census_data)\n\ntod &lt;- centroids[st_intersects(centroids,metro_union_buffer) %&gt;% lengths &gt; 0, ] %&gt;%\n  st_drop_geometry() %&gt;%\n  left_join(all_census_data) %&gt;%\n  st_sf() %&gt;%\n  mutate(type='tod')\n\nnon_tod &lt;- centroids[st_disjoint(centroids,metro_union_buffer) %&gt;% lengths &gt; 0, ] %&gt;%\n  st_drop_geometry() %&gt;%\n  left_join(all_census_data) %&gt;%\n  st_sf() %&gt;%\n  mutate(type='non-TOD')\n\nall_census_data &lt;- rbind(tod, non_tod)\n\ntod_dissolve &lt;- st_union(tod)\n\ndcmetro$tod_change[dcmetro$NAME %in% c(\"Columbia Heights\",\"Georgia Ave-Petworth\",\"Brookland-CUA\",\"NoMa-Gallaudet U\")] &lt;- 'Yes'"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#mean-annual-income-in-the-tod",
    "href": "posts/TOD_Washington_DC/DCTOD.html#mean-annual-income-in-the-tod",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Mean Annual Income in the TOD",
    "text": "Mean Annual Income in the TOD\nThe maps below show the mean annual income by census tract in 2009 and 2019 - the TOD area is indicated by the red outline and metro stops are shown as black dots. The 2009 data is adjusted for inflation, and income data is presented using the value of the dollar in 2017. Income in Washington DC is generally highest in the Northwestern areas of Washington DC, while census tracts West of the Anacostia river have the lowest annual income. Annual income has increased in several census tracts located within the TOD. Metro stops where there is a notable increase in income in the census tracts surrounding the metro stop are visualized in the map as larger dots. Areas of the TOD where we observe a large increase in income levels from 2009 to 2017 include census tracts near the Columbia Heights and Petworth metro stops on the green metro line, and tracts located near the Brookland and NOMA metro stops on the red line.\n\n\nCode\nall_census_data &lt;- all_census_data %&gt;% \n  mutate(income_class = cut(median_income, breaks = c(0, 25000, 50000, 75000, 100000, max(all_census_data$median_income, na.rm=TRUE))))\n\nggplot()+\n  geom_sf(data = all_census_data,linewidth=0.1,color='grey70',aes(fill = income_class))+\n  geom_sf(data = dcwater,fill='#5ee4ff',color=NA)+\n  geom_sf(data = tod_dissolve,fill='transparent',color='red',linewidth=1)+\n  geom_sf(data = dcmetro,size=1.3)+\n  geom_sf(data = dcmetro %&gt;% filter(tod_change == 'Yes'),size=3,fill='gray50')+\n  ggtitle('Median Annual Income')+\n  facet_wrap(~year)+\n  scale_fill_manual(values = palette5,\n                    name = \"Income (USD)\",\n                    na.value = 'grey80',\n                    labels = c('0 - 25,000','25,001 - 50,000','50,001 - 75,000','75,0001 - 100,000', '&gt;100,000','No Data'))+\n  mapTheme()"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#median-rent-in-the-tod",
    "href": "posts/TOD_Washington_DC/DCTOD.html#median-rent-in-the-tod",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Median Rent in the TOD",
    "text": "Median Rent in the TOD\nThe maps below show the median monthly rent by census tract in 2009 and 2017. As in the previous map, the TOD area is shown by a red outline and metro stops are shown as black dots. Rent has increased sharply in many neighborhoods in Washington DC from 2009 to 2017. The 2009 map has been adjusted for inflation, and all data is represented using the value of a dollar in 2017. Rent has increased from 2009 to 2017 in several areas of Washington DC - some of the largest increases in rent have occurred in neighborhoods located in the TOD area. Notably, the census tracts located near the Columbia Heights, Petworth, Brookland, Fort Totten, and Takoma metro stops have all experienced increases in rent - in these neighborhoods residents appear to be willing to pay more in rent to live near the metro. However, the pattern of rising rents in the TOD area is not present in all parts of the city, residents living east of the Anacostia river continue to have some of the lowest rents in Washington DC. The cost of rent east of the Anacostia is similar in the TOD and non-TOD areas.\n\n\nCode\nall_census_data &lt;- all_census_data %&gt;% \n  mutate(rent_class = cut(median_rent, breaks = c(0, 500, 1000, 1500, 2000, max(all_census_data$median_rent, na.rm=TRUE))))\n\nggplot()+\n  geom_sf(data = all_census_data,linewidth=0.1,color='grey70',aes(fill = rent_class))+\n  geom_sf(data = dcwater,fill='#5ee4ff',color=NA)+\n  geom_sf(data = tod_dissolve,fill='transparent',color='red',linewidth=1)+\n  geom_sf(data = dcmetro,size=1.3)+\n  ggtitle(\"Median Monthly Rent\")+\n  facet_wrap(~year)+\n  scale_fill_manual(values = palette5,\n                    name = \"Rent (USD)\",\n                    na.value = 'grey80',\n                    labels = c('&lt;=500','501 - 1,000','1,001 - 1,500','1,501 - 2,000', '&gt;2,000'))+\n  mapTheme()"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#public-transit-usage-in-the-tod",
    "href": "posts/TOD_Washington_DC/DCTOD.html#public-transit-usage-in-the-tod",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Public Transit Usage in the TOD",
    "text": "Public Transit Usage in the TOD\nThese maps show the percent of workers using any form of public transit to commute to work in 2009 and 2017. As in the previous maps, TOD area is shown by the red line and metro stops are presented as block dots. The percentage of workers commuting to work on public transit is higher in the TOD area. The may be partially attributed to the faster speed and greater reliability of metro compared to the bus. Residents who have the ability to live in desirable areas with high rent and want to commute by public transit appear to be choosing to live near metro stops. For instance, areas with high rents along the green line and both branches of the red line also have a high percentage of residents commuting to work on public transit.\nMetro stops in the downtown corridor do not have a high percentage of the population living near the metro stops commuting to work on public transit. This may be because residents who live downtown are likely to also work downtown and live close enough to their workplace to walk to work. However, additional analysis is needed to confirm this hypothesis. In areas with high income which are also located outside of the TOD-area residents are not likely to commute to work on public transit.\n\n\nCode\nall_census_data &lt;- all_census_data %&gt;% \n  mutate(transit_class = cut(pct_transit, breaks = c(0, 20, 30, 40, 50, max(all_census_data$pct_transit, na.rm=TRUE))))\n\nggplot()+\n  geom_sf(data = all_census_data,linewidth=0.1,color='grey70',aes(fill = transit_class))+\n  geom_sf(data = dcwater,fill='#5ee4ff',color=NA)+\n  geom_sf(data = tod_dissolve,fill='transparent',color='red',linewidth=1)+\n  geom_sf(data = dcmetro,size=1.3)+\n  ggtitle(\"Percent of Workers Commuting on Public Transit\")+\n  facet_wrap(~year)+\n  scale_fill_manual(values = palette5,\n                    name = \"Percent\",\n                    na.value = 'grey80',\n                    labels = c('&lt;=20%','21 - 30%','31 - 40%','41 - 50%', '&gt;50%'))+\n  mapTheme()"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#public-administration-workers",
    "href": "posts/TOD_Washington_DC/DCTOD.html#public-administration-workers",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Public Administration Workers",
    "text": "Public Administration Workers\nPublic sector jobs represent a large employment sector in the DMV area, accounting for 15 percent of jobs. Public administration works tend to have higher incomes than workers in the service industry, but lower incomes than employees working in office jobs in the private sector. Federal employees also receive a commuter benefit which can offset the cost of commuting to work on public transit. Looking at where public administrative workers are living can help us determine if federal workers are using this benefit.\nFederal workers appear to be choosing to live within the TOD area in close proximity to metro - federal commuter benefits may be contributing to this choice. Additionally, the percentage of federal workers living in the downtown corridor has declined from 2009 to 2017. As shown in previous maps, the downtown corridor has some of the highest rents in the city and public administration workers may be getting priced out of these areas as rent continues to increase. Conversely, the percentage of federal workers living near the Brookland, NOMA, and Fort Totten metro stops has increased in 2017 compared to 2009. As shown in the income and rent maps, these neighborhoods have experienced increases in rent and average income level, but rent remains lower than the most expensive parts of the city. These recently developed areas are suitable for public administration workers because the neighborhoods are desirable and provide easy access to public transit while still being affordable to middle income residents such as public administration workers.\n\n\nCode\nall_census_data &lt;- all_census_data %&gt;% \n  mutate(public_class = cut(pct_public, breaks = c(0, 10, 15, 20, 25, max(all_census_data$pct_public, na.rm=TRUE))))\n\nggplot()+\n  geom_sf(data = all_census_data,linewidth=0.1,color='grey70',aes(fill = public_class))+\n  geom_sf(data = dcwater,fill='#5ee4ff',color=NA)+\n  geom_sf(data = tod_dissolve,fill='transparent',color='red',linewidth=1)+\n  geom_sf(data = dcmetro,size=1.3)+\n  ggtitle(\"Percent of Workers in Public Adminstration\")+\n  facet_wrap(~year)+\n  scale_fill_manual(values = palette5,\n                    name = \"Percent\",\n                    na.value = 'grey80',\n                    labels = c('&lt;=10%','11 - 15%','16 - 20%','21 - 25%', '&gt;25%'))+\n  mapTheme()"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#charts",
    "href": "posts/TOD_Washington_DC/DCTOD.html#charts",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Charts",
    "text": "Charts\nThe charts below show the annual income, monthly rent, percent of workers commuting to work on public transit, and the percent of workers working public administration. For all indicators, the charts display the average value for census tracts in the TOD and the average value for census tracts in the Non-TOD areas in 2009 and 2017. Notably, the mean rent in TOD and non-TOD areas was similar in 2009. By 2017, the difference in rent between TOD and non-TOD had increased dramatically. The same patterns have also occurred when looking at annual income, with the gap in income between TOD and non-TOD areas increasing between 2009 and 2017.\n\n\nCode\ncensus_data_summary &lt;- \n  st_drop_geometry(all_census_data) %&gt;%\n  group_by(year, type) %&gt;%\n  summarize(median_income = round(mean(median_income, na.rm = T),2),\n            median_rent = round(mean(median_rent, na.rm = T),2),\n            percent_transit = round(mean(pct_transit, na.rm = T),0),\n            percent_public = round(mean(pct_public, na.rm = T),0))\n\ncensus_data_summary %&gt;%\n  gather(Variable, Value, -year, -type) %&gt;%\n  ggplot(aes(year, Value, fill = type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~Variable, scales = \"free_y\", ncol=4, strip.position = 'left',\n             labeller = as_labeller(c(median_income = 'Mean Annual Income (USD)',median_rent = 'Mean Monthly Rent (USD)',percent_transit='% Commuting on Public Transit',percent_public='% of Workers in Public Admin.'))) +\n  scale_fill_manual(values = c(\"#bae4bc\", \"#0868ac\")) +\n  labs(title = \"Indicator differences across time and space\") +\n  theme_bw()+\n  ylab(NULL) +\n  xlab('Year')+\n  scale_y_continuous(labels = function(x) format(x, big.mark = \",\",scientific = FALSE))+\n  theme(strip.background = element_blank(),\n           strip.placement = \"outside\")"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#table",
    "href": "posts/TOD_Washington_DC/DCTOD.html#table",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Table",
    "text": "Table\nBoth rent and income have increased sharply in TOD areas from 2009 and 2017. As shown in the table below the difference in mean income between TOD and non-TOD areas was only 1,548 USD in 2009 - by 2017 the difference had increased to 16,085 USD, indicating an increase of over 900%. Similar trends are present for rent where the difference in monthly rent between TOD and non-TOD areas in 2009 was only 58 USD. By 2017 the difference had increased to 252 USD, an increase of 334%.\n\n\nCode\ncensus_data_summary %&gt;%\nkable(booktabs = TRUE,col.name = c('Year','Type','Mean Income (USD)','Mean Rent (USD)','Mean % Commuting on Transit','Mean % Public Workers'), align='c') %&gt;%\n  kable_styling()\n\n\n\n\n\nYear\nType\nMean Income (USD)\nMean Rent (USD)\nMean % Commuting on Transit\nMean % Public Workers\n\n\n\n\n2009\nnon-TOD\n70678.70\n1037.85\n35\n16\n\n\n2009\ntod\n72443.08\n1103.85\n39\n17\n\n\n2017\nnon-TOD\n76393.21\n1285.75\n33\n15\n\n\n2017\ntod\n92478.31\n1537.99\n39\n17"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#population-map",
    "href": "posts/TOD_Washington_DC/DCTOD.html#population-map",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Population Map",
    "text": "Population Map\nThe map below shows the population living in the TOD area surrounding each metro stop - a larger bubble indicates a larger population living in the census tracts which form the TOD area surrounding the metro stop. The population living near metro stops is high in the urban downtown core of the city where large apartment buildings that house more people tend to dominate the housing landscape. In areas on the periphery of the city single family homes tend to be the main form of housing - as a result fewer residents live within the TOD at the edges of the city.\nThe areas surrounding metro stops have experienced population growth in many parts of Washington DC. This growth is most notable around the U street, Shaw, and Columbia Heights metro stops on the green line and is likely a result of new apartment buildings being developed around metro stops as part of a TOD development strategy.\n\n\nCode\nggplot()+\n  geom_sf(data = all_census_data,linewidth=0.3,color='#bcbcbc',fill='#eaeaea')+\n  geom_sf(data = dcwater,fill='#5ee4ff',color=NA)+\n  geom_sf(data = tod_dissolve,fill='transparent',color='red',linewidth=0.5)+\n  geom_sf(data = dcmetro, color='blue', aes(size=Population))+\n  ggtitle(\"Population Living in Within Half Mile of Metro Stop\")+\n  scale_size(range = c(0.1, 4))+\n  scale_fill_manual(name='Population')+\n  mapTheme()+\n  facet_wrap(~year)"
  },
  {
    "objectID": "posts/TOD_Washington_DC/DCTOD.html#rent-map",
    "href": "posts/TOD_Washington_DC/DCTOD.html#rent-map",
    "title": "Analysis of Transit Oriented Development Policies in Washington DC",
    "section": "Rent Map",
    "text": "Rent Map\nThe increase in population around the Columbia Heights, Petworth, U Street, and Shaw metro stops has been accompanied by a dramatic increase in rent. Rent in the census tract around the Shaw metro stop doubled from 2009 to 2017. Similar trends are present in the TOD surrounding other metro stop such as Brookland, Fort Totten, and Petworth which experienced rent increases of 41%, 81% and 58%. Rent in TOD areas East Of Anacostia river have also increases, but by much smaller percentages. Rent in census tracts bordering the Congress Heights metro stop increased by just 7% while rent in census tracts around the Anacostia metro stop increased by 20%.\n\n\nCode\nggplot()+\n  geom_sf(data = all_census_data,linewidth=0.3,color='#bcbcbc',fill='#eaeaea')+\n  geom_sf(data = dcwater,fill='#5ee4ff',color=NA)+\n  geom_sf(data = tod_dissolve,fill='transparent',color='red',linewidth=0.5)+\n  geom_sf(data = dcmetro, color='blue', aes(size=Rent))+\n  ggtitle(\"Median Rent Within a Half Mile of Metro Stop\")+\n  scale_size(range = c(0.01, 3))+\n  mapTheme()+\n  facet_wrap(~year)"
  }
]